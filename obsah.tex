\chapter{Introduction}

The importance of computers in our everyday life has largely increased over the past few decades.
A~lot of us can only hardly imagine doing their jobs without a help of an appropriate computer program
and we also spend a plenty of time using computers (e.g. personal computers or mobile devices) in leisure time.
The~computer programs are also often used in very critical instances like autopilot in an airplane.
But the growing number of applications of computer programs brings also the~need for their greater safety and security.

However, a guarantee of software correctness is not an easy task
because the programs often go through many states during the computation
and it could be very time and space consuming or even impossible to check whether no undesirable behavior
may appear in any of those states.
One of the approaches to ensure the software quality is \emph{testing} (and dynamic analysis) which is based
on running the program, in the different contexts, with the different inputs
and matching a program behavior and the outputs with the expected ones.
This method can satisfy many of the requirements for the software quality and often covers the great space of the program behaviors.
On the~other side, it is only possible to prove presence of the~errors using testing not their absence \cite{dijkstra}.
Moreover finding some errors during testing does not mean that all errors are eliminated.

The mentioned weakness of testing can be resolved by \emph{formal verification}.
Formal verification aims at using rigorous mathematical methods to check whether a given system meets a given specification \cite{fav:lecture}.
There are three main branches of formal verification.
The first one is \emph{model checking} which systematically explores the states of a~model (e.g. model of a program) to
prove that the model satisfies the property.
The second one is \emph{static analysis} which is done over a source code (or some modification of it) of a system
without a need of its execution.
One of the important and very widely used approaches in static analysis is called \emph{abstract interpretation} where the analysis is performed by
applying abstract transformers corresponding to the original program semantics over an abstract domain.
The last approach is \emph{theorem proving}.
It proves the program safe in a standard mathematical way -- starting from axioms and using inference rules to
verify the properties of a given system.
Theorem proving ca be partially automated.

This work deals with a specific part of static analysis called \emph{shape analysis} which is focused on the verification of programs manipulating
complex data structures (like the different kinds of lists and trees), typically allocated on the heap.
The properties checked are for example checking whether no dangling
pointers are dereferenced (no invalid dereferences), whether all allocated memory on a heap is also freed
during the program execution (no memory leaks) or whether there is not freed pointer without assigned memory (no invalid free).
There are different approaches to this kind of static analysis with the different advantages.
For example, the approach based on \emph{separation logic} \cite{seplog,seplog07} provides great scalability of a verification procedure.
On the other side, the automata based approach, particularly \emph{abstract regular tree model checking} (ARTMC) \cite{artmc}, is
superior in its flexibility and generality.
This work focuses on the verification procedure based on the concept of forest automata (FA) which
combines benefits of the both mentioned approaches.

Forest automata (FA) have been introduced in \cite{forester11} and
they are an extension of finite automata or, more precisely, extension of finite tree automata (TA).
They are used as an abstract domain in a verification procedure which performs symbolic execution of the analyzed program is performed.
A~prototype of this verification procedure has been implemented in a tool called \emph{Forester} \cite{www:forester}.
Forester verifies programs written in C~language and it detects safety violations like invalid dereferences, invalid frees,
memory leaks and also reachability of an error program location.
It is able to verify non-trivial data structures like skip-lists of the~second and the~third level.
However, the current implementation is far from perfect.
E.g., Forester currently does not support the complete C language syntax and %TODO
it is not also yet possible to check whether a found error is real or spurious.
It is also needed to refactor some parts of Forester code before any extension of the tool.
A~general goal of this work is to improve Forester in the areas described further.

Since FA are highly related to the finite tree automata, the first goal of this work is to change
the~underlying implementation of finite tree automata in Forester to \vata\ -- a state-of-the-art library for TA manipulation \cite{libvata}.
Particularly, this consists creating an interface between Forester and VATA to employ VATA as a TA library for Forester backend
which should bring better maintainability and modularity than having a special TA library implementation within Forester as it is now.
The second goal of this work is to design and implement \emph{backward run} for FA based verification
which enables checking spuriousness of an error found in a program.
The error could be spurious because too strong abstraction over FA is used.
The information gained by backward run could be used for the refinement of \emph{predicate abstraction} (one kind of abstraction over FA)
to prevent verification procedure from getting the same spurious error again.
This gradual refinement is known as \emph{counterexample-guided abstraction refinement} (CEGAR) \cite{cegar}.
However, Forester does not currently use the mentioned predicate abstraction but \emph{height abstraction} %TODO
because predicate abstraction needs backward run to be fully functional.
Height abstraction is less precise and less flexible compared to predicate abstraction.
Implementing predicate abstraction enables analysis of even more complex data structures like red-black trees.
A~part of the second goal is an implementation of predicate abstraction using backward run in Forester.

The outline of this document is following.
In Chapter \ref{ch:prel} the preliminaries are given.
The verification procedure based on FA is covered in Chapter \ref{ch:fav}.
Chapter \ref{ch:tools} provides description of \vata\ and Forester tool and Chapter \ref{ch:fova} describes an implementation of the version of Forester tool using \vata.
The design of backward run for FA base verification is given in Chapter \ref{ch:backward} and its implementation is documented
in Chapter \ref{ch:impl}.
Finally,
Chapter \ref{ch:eval} contains an overview of evaluation and 
Chapter \ref{ch:concl} summarizes this master thesis.

\chapter{Preliminaries}
\label{ch:prel}

This chapter contains the definitions of the concepts further used in this thesis.
First, graphs, trees and forests are defined together along the automata accepting them in Section \ref{sec:graph}.
Then the previously defined concepts are further extended to hierarchical ones in Section \ref{sec:fah}.
This section follows the definitions and a the structure used in \cite{techrep}.

\section{Graphs, Trees and Forests}
\label{sec:graph}

Assume a word $w = a_1 \cdots a_n$, we denote $i$-th symbol of $w$ as $a_i$.
We denote $dom(f)$ the domain of a total mapping $\funcdecl{f}{A}{B}$ and its range is denoted by $rng(f)$.

\subsection{Graphs and Trees}
\label{subsec:graph}
A \emph{ranked alphabet} is a finite set of symbols $\Sigma$ and a related mapping $\funcdecl{\#}{\Sigma}{\mathbb{N}}$
assigns to a symbol its rank.
A (directed, ordered, labelled) \emph{graph} is a total map $\funcdecl{g}{V}{\Sigma \times V^{*}}$ where $V$ is a finite set of nodes.
The items of the set $\Sigma$ are in context of the graphs called \emph{labels}.
Map $g$ maps each node $v\in V$ to:
\begin{enumerate}
	\item a label $\alpha \in \Sigma$ that we denote by $l_g(v)$.
	\item a sequence of \emph{successors} $(v_1 \cdots v_n) \in V^n$ for $n \in \mathbb{N}$.
		We denote successors by $S_g(v)$ and $v_i$ is denoted by $S^i_g(v)$.
\end{enumerate}
It holds that $\#(l_g(v)) = |S_g(v)|$.
We will omit subscript $g$ when no ambiguity is possible.

A \emph{leave} of $g$ is a node $v \in V$ such that $S_g(v) = \epsilon$.
An \emph{edge} of $g$ is a pair $v \mapsto (a, v_1 \cdots v_n))$ where $v, v_1, \ldots, v_n \in V$,
$a \in \Sigma$ such that $g(v) = (a, v_1 \cdots v_n)$.
\emph{In-degree} of a node $v' \in V$ in graph $g$ is the overall number of its occurrences in $g(v)$ for any $v \in V$.
We denote in-degree of a node $v \in V$ by $idg_g(v)$ and we omit again subscript $g$ whenever it is possible.
More formally, in-degree is defined as $idg(v') = |\{v \mapsto (a, v_1 \cdots v_n) \,|\, v \mapsto (a, v_1 \cdots v_n)
\emph{ is an edge such that } \exists i \in \{1,\ldots,n\}: v' = v_i\}|$.
The \emph{joins} of $g$ are nodes $v \in V$ such that $idg(v') > 1$.

A \emph{path} from $v\in V$ to $v' \in V$ is a sequence $p=v_0, i_1, v_1, \ldots, i_n, v_n$ where $v=v_0, v' = v_n$
and $\forall j \in \{1,\ldots,n\}: v_j = S^{i_j}(v_{j-1})$ (informally, $v_j$ is the $i_j$-th successor of $v_{j-1}$).
The empty path has $n=0$.
The path $p$ has \emph{length} $n$ what we denote by $length(p) = n$.
The path $p$ is acyclic if $\forall u_i,u_j \in p: i \neq j \Rightarrow u_i \neq u_j$.
A \emph{cost} of an acyclic path is sequence $i_1, \ldots, i_n$.
The path $p$ is \emph{cheaper} than another path $p'$ iff the cost of $p$ is lexicographically smaller than that of $p'$. 
A node $u \in V$ is \emph{reachable} from a node $v \in V$ iff that is a path from $v$ to $u$ or $u=v$.
A node $u \in V$ is a \emph{root} of $g$ iff all nodes $v \in V$ are reachable from $u$.
We use the term $root$ also for a mapping $\funcdecl{root}{g}{V}$ which maps a graph to its root.
When there is a root in a graph then the graph is called \emph{rooted}.

A \emph{tree} $t$ is a graph which is either empty, or it has exactly one root and $\forall v \in V: idg(v) \leq 1$ (informally,
each node is a successor of at most one of the other nodes).

\bexmp
We illustrate some terms about graphs and trees related to a graph $t$ in Figure \ref{fig:graph_tree}.
There is a set of nodes $V=\{v_1,v_2,v_3,v_4,v_5\}$ and
an alphabet $\Sigma = \{a,b\}$ with a ranking function $\#$ such that $\#(a) = 2$ and $\#(b) = 0$.
Then the graph $t$ is mapping $t(v_1) = (a, (v_2,v_3))$, $t(v_2) = (b, ())$,
$t(v_3) = (a, (v_4, v_5))$, $t(v_4) = (b, ())$, $t(v_5) = (b, ())$.
The leaves of $t$ are $v_2, v_4, v_5$.
The edges correspond to the definition of $t$ so there is for example
edge $v_1 \mapsto (a,(v_2,v_3))$.
There different paths but to illustrate the concept of path we give
definition of path from $v_1$ to $v_5$ which is $v_1, 2, v_3, 2, v_5$.
Thus we can say that $v_5$ is reachable form $v_1$.
Since all nodes are reachable from $v_1$ then $v_1$ is a root of $t$.
Finally, no node in $s$ has more the one incoming edge hence $t$ is a tree.

	\begin{figure}[bth]
		\begin{center}
			\input{fig/prel/tree.tikz}
		\end{center}
		\caption{A graph $t$ that has attributes of a tree.}
		\label{fig:graph_tree}
	\end{figure}
	\label{ex:graph}
\eexmp

\subsection{Forests}
\label{subsec:forests}

Let us suppose without loss of generality that $\Sigma \cap \mathbb{N} = \emptyset$.
A $\Sigma$-labelled \emph{forest} is a sequence of trees $t_1 \cdots t_n$ over ($\Sigma \cup \{1,\ldots,n\}$)
where $\forall i \in \{1,\ldots,n\}: \#i = 0$.
\emph{Root references} are leaves labelled by $i \in \mathbb{N}$.
The forest $t_1 \cdots t_n$ (we suppose that the sets of nodes of the trees are disjoint) represents the graph $\fagr$ that could
be constructed by interconnecting roots by the related root reference.
E.g., a root reference $2$ in $t_1$ would be replaced by the root node of $t_2$.
Let's formalize the idea of construction of $\fagr$.
$\fagr$ contains an edge $v \mapsto (a,v_1 \cdots v_m)$ iff $\exists i \in \{1, \ldots, n\} \ \exists(v \mapsto (a, v_1' \cdots v_m')) \in edges(t_i)
\ \forall j \in \{1,\ldots,m\}: v_j = h(v_j')$ where $edges(t_i)$ is the set of all edges of the tree $t_i$ and
\[ h(v_j') = \left\{
  \begin{array}{l l}
  root(t_k) & \quad \text{if $v_j'$ is a root reference with $l(v_j') = k$}\\
  v_j'   & \quad \text{otherwise}
  \end{array} \right.\]

\pagebreak
\bexmp
We illustrate the notion of forest in an example.
Consider a forest $f$ in Figure \ref{fig:forest}.
The forest $f$ consists three trees, $t_1$ with a root $u_1$,
$t_2$ with a root $v_1$ and $t_3$ with a root $w_1$.
The alphabet $\Sigma$ of the trees is same as in Example \ref{ex:graph} but $f$
is defined over $\Sigma \cup \{\overline{2}, \overline{3}\}$
where $\overline{2}, \overline{3}$ denotes the root references $u_5$, $v_3$.
to the second and the third tree.

We can obtain a graph $\otimes t_1,t_2,t_3$ shown in Figure from $t_1, t_2, t_3$.
The edges with root references are replaced by another edges leading to the roots
of the referenced trees.
A created graph is in Figure \ref{fig:forest_graph}.

	\begin{figure}[bth]
	\begin{center}
		\scalebox{1}
		{
			\input{fig/prel/for1.tikz}
			\hspace{0.55cm}
			\input{fig/prel/for2.tikz}
			\hspace{0.55cm}
			\input{fig/prel/for3.tikz}
		}
		\caption{A forest $f$ consisting the three trees $t_1, t_2, t_3$ with roots $u_1, v_1, w_1$}.
	  \label{fig:forest}
	\end{center}
	\end{figure}

	\begin{figure}[bth]
	\begin{center}
		\input{fig/prel/for_graph.tikz}
		\caption{A graph $\otimes t_1,t_2,t_3$ obtained from a forest in Figure \ref{fig:forest}.
		The green edges are the ones added to create the graph from forest}.
	  \label{fig:forest_graph}
	\end{center}
	\end{figure}
\eexmp

\subsection{Graphs and Forests with Ports}
\label{subsec:gfp}

We extend graphs and forests with concept of \emph{ports} which should
serve for marking input and output nodes.
An \emph{input-output-graph} (io-graph) is a pair $(g,\phi)$ (in sense of brevity also denoted by $g_\phi$)
where $g$ is a graph and $\phi=(\phi_1 \cdots \phi_n) \in dom(g)^+$ is a sequence of ports, $\phi_1$
is an input port and $\phi_1 \cdots \phi_n$ is a sequence of output ports.
The ports in $\phi$ are unique.
If a root of $g_\phi$ is $\phi_1$ then $g_\phi$ is called \emph{accessible}.

The \emph{cut-points} of a graph $g_\phi$ is the set of its ports and joins and we denote this set by $cps(g_\phi)$.
Formally, $cps(g_\phi)=\{v \in V\,|\, v \in \phi \vee idg(v) > 1\}$.

An \emph{io-forest} is a pair $f=(t_1 \cdots t_n, \pi)$ such that $n \geq 1$ and $\pi \in \{1,\ldots,n\}^n$
is a sequence of port indices, where $\pi_1$ is a input port index and $\pi_2 \ldots \pi_{|\pi|}$ is a sequence
of the output ports indices.
As in the case of the ports the indices are unique.
It is also possible to construct an io-graph $\otimes f$ from a forest $f$ such that
$\otimes f = (\otimes t_1 \cdots t_n,root(t_{\pi_{1}},\ldots,t_{\pi_{n}}))$.
So the ports of $\otimes f$ are roots of the trees indexed by indices in $\pi$.
This means that a root of the tree pointed by $\pi_1$ is an input port of $\otimes f$ and
the roots of the trees pointed by the other indices in $\pi$ are the output ports of $\otimes f$.

\bexmp
Recall graph $t$ from Figure \ref{fig:graph_tree}.
We can extend it to a io-graph $t_\phi$ where $t$ is unchanged and
$\phi=(v_1,v_4,v_5)$ (of course, there are different ways how to define the ports).
The io-graph $t_\phi$ has an input port $v_1$ and the output ports are $v_2,v_3$.
Because $v_1$ is the root the tree $t_\phi$ is accessible.
The cut-points of $t_\phi$ are $v_1, v_4, v_5$.

In Figure \ref{fig:forest} we shown the forest $f$.
This forest could be extended to a io-forest $f_{io}=((t_1,t_2,t_3),\pi)$ by defining sequence of port indices $\pi$
which could be e.g., $(1,3)$.
Then a graph $\otimes f_{io}$ is a pair $(\otimes (t_1,t_2,t_3)$, $(u_1,w_1))$.
The graph $\otimes (t_1,t_2,t_3)$ is the same as in Figure \ref{fig:forest_graph}.
The input port $u_1$ is related to the input port index $1$ of $f_{io}$
and the output port $w_1$ is related to the output port index $3$.
\label{ex:iograph}
\eexmp

\subsection{Minimal and Canonical Forest}
\label{subsec:mcforest}

There are two properties of the forests, minimality and canonicity, that we will further employ.
The properties make possible to represent an io-forest in unique way.
That enables manipulating with io-forest it in deterministic way.
An io-forest $f_m=(t_1 \cdots t_n, \phi)$ representing a graph $\otimes f$ is \emph{minimal}
iff the roots of trees $t_1,\ldots,t_n$ corresponds to the cut-points of $\otimes f$,
so there is a bijection between $\{root(t_k)\,|\, t_k \in \{t_1, \ldots, t_n\} )\}$ and $cps(\otimes f)$.
The minimal io-forest is unique up-to to permutations of $t_1,\ldots,t_n$.

We need to defined ordering $\preceq_p$, so called \emph{canonical ordering}, over cut-points of $\otimes f$ to be able to define canonicity of the forest $f$.
The canonical ordering $\preceq_p \subseteq cps(\otimes f) \times cps(\otimes f)$ is defined as follows: $c_1 \preceq_p c_2 \Leftrightarrow \emph{the cost of the cheapest path from }
\phi_1 \emph{ (input port) to } c_1 \emph{ is}$ $\emph{cheaper}$ $\emph{ than the cost of the cheapest path from } \phi_1 \emph{ to } c_2$.
The io-forest $f_c$ is \emph{canonical} iff it is minimal, the trees $t_1,\ldots, t_n$ are ordered by $\preceq_p$, and $\otimes f$ is accessible.
The canonical io-forest is a unique representation of a accessible io-graph.
The canonical io-forest can be obtained by depth-first traversal (DFT)\cite{taocp} of $\otimes f$.
We need to assume that there is an ordering $\leq_\Sigma$ over labels $\Sigma$ of $\otimes f$ before description of DFT method to
make DFT traversal deterministic and hence the ordering of the trees unique.
We create a stack of nodes initialized with input and output ports ordered by $\preceq_p$ when the smallest node is on the top of the stack.
Then the DFT is run over $\otimes f$ and we obtain canonical forest $f_c$ where trees are in the following order.
The first ones are trees corresponding to the ports of $f$ ordered by $\preceq_p$ and the rest of the trees are in the order
in which they were visited by DFT traversal.


\subsection{Tree Automata}
\label{subsec:ta}

A (finite, non-deterministic, top-down) \emph{tree automaton} (TA) is a
quadruple $A = (Q, \Sigma, \Delta, R)$ where
\begin{itemize}
	\item $Q$ is a finite set of \emph{states}
	\item $\Sigma$ is a ranked alphabet
	\item $\Delta$ is a set of \emph{transition rules} where transitions have a form $(q,a,q_1 \cdots q_n)$ where $q,q_1,\ldots,q_n \in Q$, $a \in \Sigma$, $n \geq 0$ and $\#a = n$.
		Alternatively, we write $q \xrightarrow{a} (q_1 \cdots q_n)$ to denote a $(q,a,q_1 \cdots q_n) \in \Delta$.
		When $n=0$ then the rule is called \emph{leaf rule}.
	\item $R \subseteq Q$ is a set of \emph{root states}.
\end{itemize}

We could symmetrically define also bottom-up tree automata as quadruple $B = (Q, \Sigma, \Delta, F)$ where
\begin{itemize}
	\item $Q$ is a finite set of states
	\item $\Sigma$ is a ranked alphabet
	\item $\Delta$ is a set of transition rules where transition has a form $(q_1 \cdots q_n,a,q)$ where $q,q_1,\ldots,q_n \in Q$, $a \in \Sigma$, $n \geq 0$ and $\#a = n$.
		We can again write $(q_1 \cdots q_n) \xrightarrow{a} q$ to denote a $(q_1 \cdots q_n,a,q) \in \Delta$.
		When $n=0$ then the rule is called \emph{leaf rule}.
	\item $F \subseteq Q$ is a set of \emph{final states}.
\end{itemize}

We will further consider top-down tree automata as default.

Now the semantics of TA will be defined.
A \emph{run} of $A$ over a tree $t$ is mapping $\funcdecl{\rho}{dom(t)}{Q}$ such that
$\forall v \in dom(t)\ \exists (q \xrightarrow{a} (q_1 \cdots q_n)) \in \Delta:  q=\rho(v) \wedge  \forall i \in \{1, \ldots, |S(v)|\}: q_i=\rho(S(v)_i)$.
We use $t \Rightarrow_{\rho} q$ to denote a run $\rho$ of $A$ over tree $t$ s.t. $\rho(root(t)) = q$ and we use $t \Rightarrow q$ to denote that there exists
run $\rho$ over $t$ to $q$.
Finally, we can define the language of $A$.
The \emph{language} of a state $q\in Q$ is defined by $L(q) = \{t\,|\, t \Rightarrow q\}$.
The \emph{language} of $A$ is defined by $L(A) = \bigcup_{q\in R} L(q)$.

\bexmp
Let define a tree automata $A=(Q,\Sigma,\Delta, R)$
where $Q=\{q_1,q_2,q_3,q_4,q_5\}$, $\Sigma = \{a,b\}$,
such that $\#(a) = 2, \#(b) =0$, $R=\{q_1\}$,
and $\Delta=\{q_1 \xrightarrow{a} (q_2,q_3), q_2 \xrightarrow{b} (),
q_3 \xrightarrow{b} (q_4,q_5), q_4 \xrightarrow{b} (), q_4 \xrightarrow{b} ()\}$.
Then a run $\rho$ of $A$ over the tree $t$ from Figure \ref{fig:graph_tree}
is defined as follows: $\forall i \in \{1,\ldots,5\}: \rho(v_i) = q_i$.
Since $\rho(root(t)) = \rho(v_1) = q_1$ then $t \in L(q_1)$ and because $q \in R$
it also holds $t \in L(A)$.
\label{ex:ta}
\eexmp

\subsection{Forest Automata}
\label{subsec:fa}

A \emph{Forest Automata} (FA) over $\Sigma$ is a pair $F=(A_1\cdots A_n, \pi)$
where $A_1 \cdots A_n$ is a sequence of tree automata defined over the alphabet $\Sigma \cup \{1,\ldots,n\}$
and $\pi = I_1 \cdots I_n$ where $I_1,\ldots, I_n \in \{1, \ldots, n\}$ is a sequence of port indices.
There are two kinds of languages related to FA.
The first one is forest language obtained by Cartesian product of the languages of particular TA (and port indices) of FA
and hence the forest language is a set of the io-forests.
The second one is the graph language obtained by connecting the io-forests from forest language to io-graphs.
Formally, the \emph{forest language} of the FA $F$ is the set of io-forests $L_f(F)= L(A_1) \times \ldots L(A_n) \times \{\pi\}$.
Note that it is necessary to add to the Cartesian product also the sequence of indices to preserve structure of io-forests
and hence to be able to construct graph language of $F$ in deterministic way.
The \emph{graph language} of $F$ is the set of io-graph $L(F) = \{\otimes f\,|\, f \in L_f(F)\}$.
We say that $F$ respects \emph{canonicity} if $\forall f \in L_f(F): \emph{f is canonical}$.

One of the most important operation over forest automata performed in analysis of a program is checking
language inclusion of two forest automata.
This operation is performed to check whether a fixpoint in a program point is reached during symbolic execution.
Checking inclusion of languages of forest automata respecting canonicity could be done \emph{component-wise},
i.e. checking language inclusion of their tree automata one by one.

\begin{lemma}
	Let $F^1 = (A_1^1\cdots A_{n_{1}}^1, \pi^1)$ and $F^2 = (A_1^2\cdots A_{n_{2}}^2, \pi^2)$
	be two FA respecting canonicity.
	Then $L(F^1) \subseteq L(F^2)$ iff
	\begin{itemize}
			\item $n_1$ = $n_2$
			\item $\pi^1 = \pi^2$
			\item $\forall i \in \{1,\ldots,n_1\}: L(A_i^1) \subseteq L(A_i^2)$
	\end{itemize}
\end{lemma}
\begin{proof}
	Proof can be found in \cite{forester:techrep}.
\end{proof}

\bexmp
We consider the io-forest $f_{io}=((t_1,t_2,t_3), (1,3))$ from Example \ref{ex:iograph}.
The tree $t_1$ (which is the same as the tree $t$) belongs to the language of TA $A$
defined in Example \ref{ex:ta}.
We further define a TA $B=(Q_B,\Sigma, \Delta_B, R_B)$ where $Q_B=\{p_1,p_2,p_3\}$,
$\Sigma$ is same as in Example \ref{ex:ta},
$\Delta=\{p_1 \xrightarrow{a} (p_2,p_3),
p_2 \xrightarrow{b} (),
p_3 \xrightarrow{b} ()\}$
and $R\{p_1\}$.
TA $B$ has $t_2$ in its language.
Finally, we define a TA $C=(Q_C,\Sigma, \Delta_C, R_C)$ where $Q_C=\{r_1\}$,
$\Sigma$ is again the same as before,
$\Delta= r_1 \xrightarrow{b} ()\}$
and $R\{r_1\}$.
A set $L(C)$ contains $t_3$.
Putting all defined automata together we can construct a forest automata $F=((A,B,C),(1,3))$.
The io-forest $f_{io}$ is in the forest language $L_f(F)$ of $F$ because it belongs
to $L(A) \times L(B) \times L(C) \times \{(1,3)\}$.
Hence the graph $\otimes f_{io} = (\otimes (t_1,t_2,t_3),(u_1,w_1))$
(also defined in Example \ref{ex:iograph}) is in the graph language $L(F)$.
\eexmp

\section{Forest Automata of Higher Level}
\label{sec:fah}

We are able to represent some data structures, like singly-linked lists or trees, by already defined forest automata
but we need to define hierarchical forest automata to be able to represent
another class of data structures (further described in \ref{subsec:boxes}), like doubly-linked lists or trees with root pointers.
For creating the hierarchy of forest automata we need to introduce \emph{structured labels}.
This labels could contain another forest automata.
The hierarchy then comes from the fact that forest automata of a certain level
are defined over structured labels containing FA of lower levels.

\subsection{Structured Labels}

$\Gamma$ is a ranked alphabet of \emph{sub-labels} with defined total ordering $\sqsubset$.
Let $g$ be a graph defined over $2^\Gamma$ where $A$ denotes a symbol of $g$ and $\forall A \subseteq \Gamma: \#A = \sum_{a\in A} \#a$.
The graph $g$ has edges in the form $v \mapsto (\{a_1,\ldots,a_m\},v_1 \cdots v_n)$ where
$a_1 \sqsubset a_2 \sqsubset \ldots \sqsubset a_m$ and $\sum_{a \in \{a_1,\ldots,a_n\}} \# a = n$.
We denote such edge by $e$.
Each edge $e$ consists \emph{sub-edges} that creates sequence $e\langle 1\rangle = v \mapsto (a_1,v_1 \cdots v_{\#a_1}) \cdots e\langle n\rangle= v \mapsto (a_m,v_{n-\#a_m+1} \cdots v_m)$.
We denote $i$-th sub-edge of $e$ in $g$ by $e\langle i\rangle = v \mapsto (a_i,v_k \cdots v_l)$ where $i \in \{1,\ldots,m\}$ or
we can also denote it by indices from $\{k,\ldots,l\}$.
We use $SE(g)$ to denote all sub-edges of graph $g$.
A node $v$ of a graph is \emph{isolated} if it is not part of any sub-edge.
Formally, a node $v$ is isolated iff $\nexists\, e\langle i\rangle = v' \mapsto (a_i,v_k \cdots v_l): v = v' \wedge \nexists\, e\langle i\rangle = v' \mapsto (a_i,v_k \cdots v_l)\ \exists v'' \in \{v_k,\ldots, v_l\}: v = v''$.
A graph $g$ is \emph{unambiguously determined} by $SE(g)$ if $g$ has no isolated nodes.

\subsection{Tree Automata over Structured Labels}

Since we already extended the labels to structures ones we also define tree automata over these labels.
A (finite, non-deterministic, top-down) \emph{tree automata} (over structured labels) is quadruple $A=(Q,2^\Gamma, \Delta, R)$ where
\begin{itemize}
	\item $Q$ is a finite set of states.
	\item $\Gamma$ is a ranked alphabet.
	\item $\Delta$ is a set of transition rules set with rules in the form $(q,\{a_1,\ldots,a_m\},q_1 \cdots q_n)$ where $q,q_1,\ldots,q_n \in Q$, $\{a_1,\ldots,a_m\} \in \Gamma$.
	Each rule could be interpreted as a sequence of the \emph{rule-terms} $d\langle 1\rangle = q \mapsto (a_1,q_1 \cdots q_{\#a_1}) \cdots d\langle n\rangle= q \mapsto (a_m,q_{n-\#a_m+1} \cdots q_n)$ and
	we denote the $i$-th rule term of sequence again by $d\langle i\rangle$ where $i \in \{1,\ldots,m\}$.
	\item $R\subseteq Q$ is a finite set of root states.
\end{itemize}

\subsection{Forest Automata of Higher Level}

Finally, we can extend also FA to a version defined over structured labels.
Informally, a forest automaton of a higher level has another forest automata (of lower lever) as the symbols on its edges.
This makes possible to build a hierarchy of forest automata.
Let formalize this idea.
We start from forest automata of \emph{level} 1.
So let $\Gamma_1$ be a set of all forest automata over $2^\Gamma$ and let the elements of this set be called \emph{forest automata of level 1}.
All forest automata of \emph{level $i$} form the set $\Gamma_i$.
A forest automaton $F$ of level $i$ is defined over the ranked alphabet $2^{\Gamma \cup \Delta}$ where $\Delta$ is a subset of forest automata of
level $i-1$ which are called \emph{boxes} of $F$.
The \emph{Rank} $F$ of an FA $F$ is the number of its output port indices.
Finally, the set of all forest automata of all levels $\sum_{i \geq 0} \Gamma_i$ is denoted by $\Gamma^{*}$ and it is ordered by a total ordering $\sqsubset_{\Gamma_*}$.

We define operation \emph{sub-edge replacement} which help us to define semantics of forest automata of higher level.
Informally, the sub-edge replacement removes a sub-edge and matches its origin and outputs with a new graph serving like a substitution of the sub-edge.
This operation will be further used for replacement of sub-edge of FA with by a graph represented by FA of lower level.

Formally, let $g$ be a graph with an edge $e \in edges(g)$ and sub-edge $e\langle i\rangle = v_1 \rightarrow (a,v_2 \cdots v_n)$.
Let $g_{\phi}'$ be an io-graph such that $|\phi| = n$.
We assume that $dom(g) \cap dom(g') = \emptyset$.
The sub-edge $e\langle i\rangle$ could be replaced by $g'$ such that $\forall j \in \{1,\ldots,n\}: l_{g}(v_j) \cap
l_{g'}(\phi_j) = \emptyset$
(this conditions checks whether there is no successor of $v_j$ and $\phi_j$ reachable over the same
label from both nodes).
The result of replacement (if it is possible to do it) is denoted as $g\subst{g'_\phi}{e\langle i\rangle}$.
The result is the graph $g_n$ in a sequence $g_0 \cdots g_n$ of graphs which are defined as follows: 
\begin{itemize}
	\item $SE(g_0) = SE(g) \cup SE(g') \setminus \{e\langle i\rangle\}$.
	\item $\forall j \in \{1, \ldots, n\}: \emph{the graph } g_j \emph{ is obtained from } g_{j-1} \emph{ by following procedure }$
		\begin{enumerate}
			\item Deriving a~graph $h$ by replacing the origin of the sub-edges of the $j$-th port $\phi_j$ of $g'$ by $v_j$.
			\item Redirecting edges leading to $\phi_{j}$ to $v_j$, i.e., replacing all occurrences of $\phi_j$ in $rng(h)$ by $v_j$
			\item Removing $\phi_j$. 
		\end{enumerate}
\end{itemize}

We apply the concept of sub-edge replacement to the forest automata of higher level now
and introduce following two procedures over FA.
\begin{itemize}
	\item \emph{Unfolding} of a graph $g$ is replacement of its sub-edge with a symbol, which is
		a FA $F'$, by a graph from $L(F')$.
		Formally, sub-edge $e\langle i \rangle$ of the graph $g$ has a symbol $a$ and this symbol is a FA $F'$ (so this symbol is a box)
		and $g'_\phi \in L(a)$ then $h = g \subst{g'_\phi}{e\langle i \rangle}$ is an unfolding of $g$.
		We denote unfolding $h$ of $g$ by $g \prec h$.
	\item \emph{Folding} is a replacement of $g'_\phi$ by $e \langle i \rangle$ in $h$ obtaining $g$.
		So we can say that $g'_\phi$ is folded to $e \langle i \rangle$. 
\end{itemize}

A transitive reflective closure of $\prec$ is denoted by $\prec^*$.
A set of all graphs obtained by repeated application of unfolding from
a graph $g$ over ranked alphabet $\Gamma$ is called \emph{$\Gamma$-semantics}. 
\emph{$\Gamma$-semantics} is formally defined as a set of graphs $g'$ such that $g \prec^* g'$.
We denote it as $\llbracket g \rrbracket_\Gamma$ or just simply $\llbracket g \rrbracket$ when it is
clear which alphabet we speak about.
Finally, \emph{$\Gamma$-semantics} is defined for a FA $F$ of higher level as follows $\llbracket F \rrbracket = \bigcup_{g_\phi \in L(F)} (\llbracket g \rrbracket \times \{\phi\})$.
Please note that the the meaning of $L(F)$ and $L_f(F)$ has not been changed so the both sets (languages) can contain just graphs (or forests)
over structured labels.

When we recall the definition of canonicity respecting FA we will find that it is applicable also for FA of higher level.
A FA $F$ is canonicity respecting if $\forall f \in L_f(F): \emph{f is canonical}$ and since definition of canonical $f$
is not affected by extending labels to the structured ones the meaning of canonicity is same as for basic FA.
The language inclusion checking is again possible in component-wise way like in the case of basic FA what is
proofed in \cite{forester:techrep}.

%TODO sets of FA. are they really needed anywhere?

\chapter{Forest Automata based Verification}
\label{ch:fav}

Consider programs manipulating dynamic data structures.
These programs change shape of a heap (and so reach another heap configuration)
by performing operations like an allocation of the new memory nodes on the heap
or removing the old ones.
Since the dynamic data structures are unbounded the number of
the reachable heap configurations is infinite.
However, it is possible to represent the infinite state space by forest automata in finite way.
Then we can perform shape analysis over the forest automata representation of the reachable
heap configurations to discover whether no undesirable behavior like a dereference of uninitialized
pointer happen.

This chapter provides an overview of the shape analysis based forest automata introduced in \cite{forester12}.
First, a heap representation using FA is described and then
we provide overview of symbolic execution that employs framework of abstract interpretation.

\section{Heap Representation}
\label{sec:hd}

It is possible to view a \emph{heap} (more precisely, a single heap configuration)
as a (directed) graph where each allocated heap cells corresponds to a node in the graph \cite{forester13}.
The heap cells consists of \emph{pointer selectors} and \emph{data selectors}.
A pointer selectors could point to another graph node or to the value \emph{null} or it could be undefined.
A data selector picks a data from some finite data domain.
%When we consider the pointer variables of a program it also holds that they can have pointer and data selectors.
We need to split a heap graph to the trees to be able to represent in by forest automata.
The splitting is done in the following manner.
We identify the heap cut-points (corresponding to the already defined term \emph{cut-points})
which are the heap cells pointed by a pointer variable or by more then one other heap cells by their pointer selectors.
The heap is then split to the trees whose roots nodes are cut-points identified in the previous step.
The pointer selectors that pointed to the cut-points are redirected to the corresponding root references which interconnects
particular tree components (and provides possibility to reconstruct the original graph again).
Finally, the created tuple of the trees is called a forest.
Please note, that the mentioned procedure corresponds to the terms defined in Chapter \ref{ch:prel}
so it is possible to achieve canonical forest as it was described in Section \ref{subsec:mcforest}.

Let us formalize the idea given above.
We denote a pointer selector by $PSel$, a data selector by $DSel$ and data domain by $\mathbb{D}$ \cite{techrep}.
A single heap configuration is a io-graph $g_{st}$ over the ranked alphabet of the structured labels from $2^\Gamma$
with sub-labels from the ranked alphabet $\Gamma = PSel \cup (DSel \times \mathbb{D})$ having the
ranking function that assigns $1$ to the pointer selectors and $0$ to the data selectors.
The values of data selectors are stored in the structured labels as sub-labels from $DSel \times \mathbb{D}$.
A node $v$ of a graph representing a heap reflects by its label the internal structure of
the related allocated memory cell in the heap so it holds that $l_g(v) \in 2^\Gamma$.
A null value is represented by a node $\texttt{null}$ with $l_g(\texttt{null}) = \emptyset$
and undefined selectors of a node $v$ have not corresponding  sub-labels in $l_g(\texttt{v})$.

\bexmp
We illustrate described principle of heap representation with an example taken from \cite{techrep}.
We consider a singly-linked list whose items are data structures containing pointers to
a next item and also integer data variable, written in C:
\begin{center}
\begin{minipage}{0.3\textwidth}
    \begin{verbatim}
     struct SLL {
      struct SLL* next;
      int data;
     };
    \end{verbatim}
\end{minipage}
\end{center}
Then a allocated cell of this singly-linked list on the heap with value $13$ in \texttt{data} variable and pointer to a cell named $s_{next}$
in the \texttt{next} variable could be represented
by a node $s$ with following label $l_g(s) = \{next_g(s_{next}),(data_g,13),()\}$.
As you can see, the pointer selector $next_g \in PSel$ represents the pointer variable \texttt{next} from \texttt{SLL} structure
has really rank $1$ and sub-label $(data_g,13) \in DSel\times \mathbb{D}$ representing \texttt{data} variable from \texttt{SLL} structure
has rank $0$.
\eexmp

The mentioned method describes how to decompose a heap graph to forest, concretely we use the io-forests.
An io-forest can be accepted by a forest automaton as a member of its language.
Moreover, a language of forest automaton is a set of such forests.
So is is possible to represent a set of the reachable heap configurations at a point of the analysed program.
Precisely, the forest automata of higher level respecting canonicity are used in the verification method
because they have richer expressive power then the non-hierarchical forest automata.
The forest automata of higher level also makes possible to represent data structure with unbounded number
of the cut-points by their folding into the boxes what is further described later.

Not only the allocated memory on the heap is modeled by forest automata during the verification procedure but also
a \emph{stack frame} of the actually analyzed function of the original program is represented by forest automata.
Particularly, the io-forests and forest automata have from their definitions exactly one (index of) input port and
just the input port $sf$ keeps a information about the stack frame.

\section{Symbolic Execution}
\label{sec:se}

So far we described representation of heap configurations using forest automata.
Now we cover how the forest automata representation is computed for each point of the analysed program.
FA-based verification procedure is a standard \emph{abstract interpretation} \cite{cousot:77}.
The concrete domain assigns to each program location a set of pairs
$(\sigma,H)$ where a mapping $\sigma$ maps each variable
to a node in $H$, to a $\texttt{null}$ or to an undefined value, and $H$ is a single heap configuration.
The abstract domain than assigns to each program location a finite set of pairs
$(\sigma, F)$ (called \emph{abstract configuration}) where $\sigma$ maps again each variable to a
$\texttt{null}$ value or to undefined value or to an index of TA in $F$ and $F$ is a forest automata
of higher level respecting canonicity representing a set of heaps configurations.
Note that a set of FA is needed in abstract domain to represent one program locations (since
abstract domain maps to the set of abstract configurations).
FA are not closed under union so it is not possible to represent the sets of heaps by single FA.

The verification starts from initial abstract configuration that consists an FA for initial heap configuration representing io-graph
$g_{\mathit{sf}}$ where $g$ consists from two nodes.
The first one is $\texttt{null}$ and the second one is empty stack frame $sf$ with $l_g(\mathit{sf}) = \emptyset$.
The sequence of \emph{abstract transformers} related to the program statements is then iteratively applied to the abstract configurations.
The process of applying abstract transformers over abstract domain is called \emph{symbolic execution}.
As it was said, each abstract transformer correspond to a statement of the analyzed program.
It is possible to define a function $f_{\texttt{op}}(g_{st})$ related to an operation \texttt{op} from the intermediate code of the analysed program.
This function models semantics of \texttt{op} in concrete domain in such way that $f_{\texttt{op}}(g_{\mathit{st}})$
returns an io-graph representing the heap configuration obtained after execution of the concrete operation \texttt{op} over concrete domain.
The abstract transformers $\tau_{\texttt{op}}$ are defined for each concrete operation \texttt{op} reflecting semantics of $f(\texttt{op})$.
The abstract transformer $\tau_{\texttt{op}}$ applied to a FA $S$ representing heap in abstract domain returns a result FA $S' = \tau_{\texttt{op}}(S)$
such that $\bigcup_{F' \in S'} \llbracket F' \rrbracket = \{ f_{\texttt{op}}(g_{sf}) \,|\, g_{sf} \in \llbracket F \rrbracket \wedge F \in S \}$.
The abstract transformer is applied separately to each forest $F \in S$.

The abstract transformers related to a program statement in a program location are applied iteratively until
abstract configurations in every program location reaches fixpoint.
Each iteration follows this procedure:
\begin{enumerate}
		\item The sets of abstract configurations at each program point are updated by applying abstract transformers following
			these steps:
			\begin{enumerate}
				\item Some boxes of FA in abstract configuration are unfold to uncover accessed part of heaps by abstract transformers.
					This step is called \emph{normalization}.
				\item The update of abstract configuration is done.
				\item The new boxes are \emph{learnt} by method described nicely in \cite{forester13}
					and these boxes are fold again.
					This is repeated until it is not possible to find the new boxes.
				\item FA in abstract configuration made canonicity respecting.
			\end{enumerate}
		\item At junctions corresponding to the loops the union is followed by \emph{widening}.
			This requires also checking language inclusion between sets of FA to check whether fixpoint has been reached.
			It is also necessary to transform the FA to canonicity respecting form before inclusion checking.
\end{enumerate}

Widening currently consists of repeating the following steps to each $F$ in the abstract configurations of a junction point until the fixpoint is reached:
\begin{enumerate}
		\item Folding boxes of $F$.
		\item Abstraction -- It is currently based on the framework of \emph{abstract regular (tree) model checking} \cite{artmc}.
			During abstraction all states $q$, $q'$ are collapsed if it holds that
			$q$ and $q'$ accepts trees with the same sets of prefixes of height at most $k$.
\end{enumerate}

It was mentioned that we need canonicity respecting FA to be able checking inclusion of languages of FA.
This is done by operation called \emph{normalization} further described in Subsection \ref{subsec:norm}.
Subsection \ref{subsec:boxes} provides brief description of cases when the boxes are applied.

\subsection{Normalization}
\label{subsec:norm}

Normalization is done over a FA $F = (A_1 \cdots A_N,\pi)$ and its result is canonicity respecting FA.
Normalization consist of the following steps:
\begin{itemize}
		\item We obtain form of $F$ in which roots of trees of transformed forests corresponds to
			cut-points in a uniform way.
			It meas that $\forall i \in \{1,\ldots,n\}$ and for all accepted forests $f_1,\ldots,f_n$ holds
			one of the following conditions:
			\begin{itemize}
				\item Root of $f_i$ is the $j-th$ cut-point in the canonical ordering of an accepted forest $\Rightarrow$
					It is the $j-th$ cut-point in the canonical ordering of all accepted forests.
				\item Otherwise, root of $f_i$ is not a cut-point of any accepted forests.
			\end{itemize}
		\item We merge $TA$ of $F$ into a form where roots of accepted forests are cut-points only.
			So when there is a TA $A$ whose accepted trees are not cut-points in $L(F)$ and there is
			a TA $B$ that contains references to the root references to $A$, then $A$ is connected
			to $B$ at places where $B$ has references to the root of $A$ so a new
			TA $B_A = (Q_A \cup Q_B, \Gamma, \Delta_{A+B}, R_B)$ is created.
			Transition rules set $\Delta_{A+B}$ is $\Delta_A \cup \Delta_B$ with substituted transition $q \rightarrow  \overline{a} (q_1 \cdots q_i \cdots q_n) \in \Delta_B$
			by $q \rightarrow  \overline{a} (q_1 \cdots q_a \cdots q_n) \in \Delta_B$, where $q_i$ is a root reference to $A$ and $q_a$ is the root of $A$. 
		\item $TA$ of $F$ are reordered by canonical ordering of cut-points.
\end{itemize}

\subsection{Boxes}
\label{subsec:boxes}

It was already explained how the heap is decomposed to the forests.
One part of the decomposition process is identifying the cut-points.
But it could happen that there is infinitely many cut-points in the graphs representing the heaps.
It is the case of e.g. doubly linked list where each node is a cut-point itself.
This could be resolved by employing boxes (box is a symbol of alphabet of forest automata of higher level which is also forest automata as it was mentioned in Chapter \ref{ch:prel}).
Since it is possible for FA to use other FA as symbols we can fold recurring sub-graphs to the boxes that are further used as the labels
and so reduce the number of cut-points (to a bounded count).
When it is needed to manipulate a graph described by a FA hidden in the box unfolding is done.
The steps of verification procedure where folding and unfolding is needed has been mentioned in the beginning of this section
and its formal definition is given in Chapter \ref{ch:prel}.
The boxes can be learn during analysing of a program automatically by the method proposed in \cite{forester13} or
it can be given to the analyzer manually by a user.

\subsection{Abstraction over Forest Automata}
\label{subsec:abstraction}

We mentioned that forest automata are able to handle also infinite state spaces
rising from a possible unboudness of the mentioned dynamic data structures.
However, there is still the problem of state explosion that often makes
nearly impossible to finish verification procedure of a system in real time
and moreover, not even termination of the verification procedure is guaranteed at all.
Therefore the abstraction over the forest automata is introduced to increase probability of termination
and to accelerate the method.
The abstraction over forest automata is based on the framework of \emph{abstract regular tree model checking} \cite{artmc}.

The abstraction accelerates the computation by merging states of forest automaton that are equivalent
to an equivalent relation over states of forest automaton.
Formally, the abstraction $\alpha$ over a tree automaton $M=(Q, \Sigma, \Delta, R)/$ is
a function $\alpha: Q \rightarrow Q/_{\eqrel{M}}$ such that $\alpha(q) = \eqclass{q}{M}$
where $\eqrel{M} \subseteq Q \times Q$ is an equivalence relation.
We denote $\alpha(M)$ the tree automaton obtained by applying $\alpha$ to $Q$.
It holds that $Q/\eqrel{M} \subseteq Q$ and also that $L(M) \subseteq L(\alpha(M))$.
Since the abstraction is defined for a single tree automaton
it is performed component-wise over a forest automaton.

The forest automata based verification currently employs \emph{height abstraction}.
This abstraction merges states with the equivalent languages upon to a given string length.
So the range of abstraction function is set of equivalence classes of relation $\heqrel{M}$
such that $\forall q_1,q_2 \in Q: q_1 \eqrel{M} q_2 \defarrow \langlen{M}{q_1} = \langlen{M}{q_2}$ where
$\langlen{M}{q}$ $=$ $\{ w \in \Sigma^n \,|\, |w| \leq n \wedge wv \in \langstate{M}{q} \wedge |v| \geq 0\}$
and $n\in \mathbb{N}$ is height of the abstraction.

The abstraction over forest automata overapproximate set of reachable
configurations of a program (what is formally expressed by $L(M) \subseteq L(\alpha(M))$).
This brings mentioned acceleration of verification procedure but on the other hand
could led to a detection of a spurious counterexample which is not present
in the original program but it occurs in a heap configuration created by abstraction.
The program verification is restarted after the detection of spurious counterexample
but it uses a refined abstraction $\alpha'$ what is abstraction such that $L(\alpha'(M)) \subseteq L(\alpha(M))$.
The refined abstraction can prevent reaching the detected spurious counterexample again.
In the case of height abstraction the refinement is done by increasing the height $n$.
However the refinement does not guarantee that the detected spurious counterexample will not
be detected again the next run.
This would be resolved by using predicate abstraction which is described later in Chapter \ref{ch:backward}
and it application to forest automata is one of the merits of this master thesis.

\chapter{The VATA Library and Forester}
\label{ch:tools}

As it was mentioned in the introduction, FA based verification is implemented by a tool
called Forester.
Since the FA are closely related to TA as it was shown in Chapter \ref{ch:prel} so
Forester also depends on an implementation of TA.
It currently has its own implementation of TA providing the operations over TA needed during verification procedure.
However it is quite impractical to maintain a special TA library inside of Forester
and it would be more practical to employ some existing efficient TA library.
The VATA library is a very efficient library which provides implementation of the standard operations over TA like union or intersection etc.,
but it also implements the state-of-the-art algorithms \cite{tacas10} for language inclusion checking which efficiency
is also crucial for performance of Forester.
It seems logical according to these facts to connect Forester with the VATA library employing VATA like a backend TA library for Forester.

This chapter provides a description of the VATA library followed by a description of Forester.

\section{\Vata}
\label{sec:VATA}

\Vata\ is open source library for nondeterministic tree automata.
Its main application is in the field of formal verification.
VATA is licensed under GPL, version 3, and can be obtained from its official website \cite{www:libvata}.
Implementation programming language is C++.
It is the only library to our knowledge implementing state-of-the-art algorithms for checking inclusion of NTA languages
what makes it suitable for use as a Forester backend library.
However, \vata\ does not only provide implementation of algorithms for NTA but also the highly efficient implementation of
algorithms for checking language inclusion of nondeterministic finite automata \cite{bt:hruska}.

\subsection{Design}
\Vata\ currently provides methods for representation of NTA in explicit encoding and also in semi-symbolic (top-down and bottom-up)
encoding using \emph{MTBDD} but it has been designed to be easy extended by other encodings (for others automata).
The library provides API for creating and manipulating NTA and also command line interface (cli) build around
the API for experimenting with a tree automata defined in a text format directly from command line.
The main concept of the design of the library is given in Figure \ref{fig:vata}.
As you can see there are the three main parts in the library design:
\begin{enumerate}
	\item \emph{Parsers} -- Parsing an input automaton from a text file.
		Timbuk \cite{timbuk} is currently the only one supported format for parsing input automata.
	\item \emph{Serializers} -- Serializing an automaton to a text file.
		Timbuk format is again the only one supported format.
	\item \emph{Automata encodings} -- The particular encodings of NTA.
		An encoding should consists of core module implementing NTA representation itself
		and also the operations over NTA in this encoding.
\end{enumerate}

\begin{figure}[bt]
\begin{center}
\input{fig/lib_design.tikz}
		\caption{The main concept of \vata. Figure is taken from \cite{libvata}}.
		\label{fig:vata}
\end{center}
\end{figure}

A \emph{program} (e.g. cli of VATA) employing the three parts of \vata\ works as follows.
An input automaton is loaded by one of the parsers to an intermediate representation.
The wrapping program chooses internal encoding of NTA to which is automaton stored from intermediate encoding (note that it is also
possible to create automaton in the chosen encoding directly using API provided by VATA).
Then the automaton is processed by applying the operations implemented by module of the chosen encoding.
Finally, automaton could be serialized to an output format.
When one wants to add her own encoding then she needs to implement only core of encoding (with API for creating automaton itself)
and needed operations over TA and can employ already implemented parses and serializers in \vata.

\subsection{Implemented NTA Encodings}

It was mentioned that there are currently implemented two representation of TA and both are described in the following subsection.

Using \emph{explicit encoding} NTA transition relation is represented by a hierarchy of the hash tables as it is shown in Figure \ref{fig:explnta}.
The first level of the hash tables hierarchy (\emph{top-level look-up tables}) maps each state $q$ of an automaton to 
a second level of the hash tables hierarchy (\emph{transition cluster}) where are stored all symbols which
are presented in the transition where $q$ is at left-handed side.
Each symbol $a$ in a transition cluster is mapped to a pointer to a set in the third level of hierarchy (\emph{sets of pointers to tuples}).
The set contains pointers to tuples which are at right-handed side in a transition with $q$ at left-handed side and with $a$ as a symbol.
The tuples are stored at a special set where every tuples is stored only once.
Note that it also possible to share part of transition relation between different automata what
brings higher efficiency in space complexity of implementation.
Module for explicit encoding also stores explicitly a set of the final states of a NFA but on
the other side it does not explicitly store a set of all states because it can be obtained from the transition relation.

\begin{figure}[bt]
\begin{center}
\input{fig/explicit.tikz}
	\caption{Explicit representation of NTA in \vata. Figure is taken from \cite{libvata}}.
	\label{fig:explnta}
\end{center}
\end{figure}

\begingroup
\tikzset{every picture/.style={scale=0.8}}%
\begin{figure}[bt]
	\centering
	\begin{subfigure}{.5\textwidth}
		\centering
		\input{fig/mtbdd_td.tikz}
		\caption{MTBDD Top-down representation of a NTA. Image is taken from \cite{libvata}.}
		\label{fig:mtbdd_td}
	\end{subfigure}%
	~
	\begin{subfigure}{.5\textwidth}
	\centering
	\input{fig/mtbdd_bu.tikz}
	\caption{MTBDD Bottom-up representation of a NTA. Image is taken from \cite{libvata}.}
	\label{fig:mtbdd_bu}
	\end{subfigure}%
\caption{Semi-symbolic encoding of tree automata.}
\label{fig:symnta}
\end{figure}
\endgroup

Another already implemented encoding is \emph{semi-symbolic} one based on VATA own implementation of MTBDD package.
This encoding is efficient mainly for TA with the large alphabets.
Because semi-symbolic encoding and MTBDD are not in the aim of this thesis they are not be described in detail
but it is possible to find deeper description in \cite{mt:lengal}.
The main principle of semi-symbolic encoding is shown in Figure \ref{fig:symnta}.
First of all it is necessary to distinguish between (a) top-down and (b) bottom-up variants of this encoding.
The first one maps each state $q$ of a NTA using MTBDD to the sets of the tuples of states such that that it is possible
to make transition from $q$ under a symbol $a$ to a tuple in appropriate set (each set of tuples is dedicated
to one symbol under which it is possible to make transition from $q$).
The former one symmetrically maps each n-tuple $(q_1 \cdots q_n)$ of a NFA using MTBDD to the sets of states
where each set $S$ is dedicated to a symbol $a$ of the NFA and contains states such that there exists a transition
with $(q_1 \cdots q_n)$ at the right-handed side and symbol $a$ and state from the set $S$ at the left-handed side.
The final state set of a NTA is again represented by explicit set in both variants,
a state set is not stored explicitly because one can obtain it from the transition relation.
the symbols are encoded (as binary strings) in MTBDD.

All of the mentioned encodings currently support efficient language inclusion checking using algorithm
from \cite{tacas10}.
On the other side the other operations are not currently implemented by all encodings.
The full enumeration of the supported operations for the particular encodings is given in Table \ref{tab:vataop}.

\input{tab/vataop.tex}

\section{Forester}
\label{sec:FA}

Forester is open source tool for verification of program manipulation complex dynamic data structures.
It currently supports program in C language.
Forester is distributed as a \emph{GCC} plugin under GPL license, version 3, and can be obtained from its official website \cite{www:forester}.
Tool is written in C++.

\subsection{Design}

Forester is implemented as a GCC plugin but it does not analyze directly intermediate code of GCC called GIMPLE but it
uses Code Listener infrastructure \cite{cl11} to provide fronted over GIMPLE.

Please see Figure \ref{fig:fa_exec} to get a high level overview of the verification process performed by Forester. 
Forester starts analysis of a program by translation of intermediate code representation provided by Code Listener
to its own microcode.
Microcode represents each program statement by one or few instructions associated abstract transformers.
Symbolic execution is then execute over this microcode.
Abstract domain represented by FA (which are also the core part of a symbolic state) is gradually transformed by the abstract transformers
represented by the microcode instructions during the symbolic execution.
When Forester detects an error the symbolic execution is aborted and the analyzed program is claimed as incorrect one.
When symbolic execution is over then Forester checks whether there is no left garbage (of course, garbage is gradually checked also during the symbolic execution)
and if it is not then a shape invariant has been found and the analyzed program is determined as correct.

\begin{figure}[bt]
	\begin{center}
		\input{fig/fa_exec.tikz}
	\end{center}
	\caption{High level overview of Forester program analysis.}
	\label{fig:fa_exec}
\end{figure}

A little deeper description of conceptual design of Forester (which is shown in Figure \ref{fig:fa_design}) and relations
between its modules is going to be given now.
Please note that implementation of Forester is not explicitly separated to the stand-alone compilation modules so the notion of the modules
used in following text is more abstract to provide reader basic summary of the Forester design.
One module could be e.g. a set of closely related classes with a similar purpose.
As it was mentioned above the Code Listener representation of GCC intermediate code is mainly used by Forester \emph{Compiler} module.
Compiler then converts Code Listener instructions to the Forest own \emph{Microcode Instructions} and creates their list over which the \emph{Symbolic Execution} is performed.
Symbolic execution then execute microcode instructions (abstract transformers) which manipulates \emph{Symbolic State} and of course
also \emph{Forest Automata} included in symbolic states.
Symbolic execution also need \emph{Symbol Context} which is created for each function (and also for global space)
and keeps information about variables used in the function, about function arguments or about stack frame layout.
A symbolic state provides information about the state of the heap which is represented by a FA and it also keeps the information about
the corresponding microcode instructions.
\emph{Forest Automata} module provides methods for manipulation FA need during verification procedure.
The operations like normalization or abstraction over FA are not part of the module containing FA implementation but are provided
like classes which take FA as parameters.
So these operation could be understood as another module \emph{Operation over Forest Automata}.
Finally, Forester currently has its own implementation of \emph{Tree Automata}.
It is very lightweighted implementation containing optimized operations directly for Forester purposes.
One of them is operation for language inclusion checking using simulation which efficiency is also crucial for Forester performance.
The advantage of this implementation is its simplicity and
the efficiency get by optimizing the implementation specially to the Forester.
On the other side, it is not easy to maintain such a optimized implementation.
Especially, when one considers that there is still progress in field of design of the efficient algorithms
and the improvement of an algorithm brings much higher efficiency then implementation optimization.

\begin{figure}[bt]
	\begin{center}
		\input{fig/fa_design.tikz}
	\end{center}
	\caption{Conceptual design of Forester.}
	\label{fig:fa_design}
\end{figure}

Please note, this is just the conceptual high level view of Forester design.
A real implementation is much more complicated (e.g. Forest automata are implemented by two classes: \emph{FA} and \emph{FAE})
and full of the technical details.
A full description of the implementation is also not the aim of this text.

It was already mentioned that substitution of Forester TA implementation by
VATA could bring some advantages.
We summarize them here again.
The first one is that it is much easier to maintain one library where
the state-of-the-art algorithms are implemented and optimized.
Since VATA and Forester is developed by the same developers it also easy to added
to VATA operations needed by Forester.
Having narrow interface between TA library and the rest of Forester
also improves code quality of Forester in the sense of modularity, maintainability and code organization.
These arguments lead us to implement a version of Forester using VATA.
This is described in Section \ref{ch:fova}.


\subsection{Implementation of Forest Automata Concepts}
\label{subsec:faimpl}

This subsection describes an implementation of the forest automata-based verification procedure in Forester.
The core of the forest automata module is class {\tt FA}.
The implementation corresponds to the definitions given in Chapter \ref{ch:prel}.
Class FA has a data member representing a vector of tree automata $(t_1 \cdots t_n)$.
It also has a data member keeping a reference to an automaton representing the global variables
and a reference to the automata representing the local variables of the actually executed function. %TODO define ABP
The labels of the TA transitions are represented by the structure {\tt NodeLabel}.
This structure contains a data member determining the type of the root node of a transition
(whether it is a general node, a data node or a node containing a vector of data).
A value related to the type of the node is also stored as a data member of the structure.
The basic operations over an FA (like adding or removing a TA from an FA) are encapsulated in class FA.
The another class providing operations over FA is class {\tt FAE}. 

As it was already mentioned, the forest automata are further used in the representation of a symbolic state.
An implementation of the representation of a symbolic state is in class {\tt SymState}.
The class keeps a reference to a Forester microcode instruction $I$ to which the symbolic state corresponds.
It has a data member that is a pointer to a forest automaton $F$ (an instance of class {\tt FAE})
representing the actual memory state.
Another data member of class SymState is a set of registers $\regsset$.
We denote set $\regsset$ by $\regs$. 
The register are local or global ones.
There are two global registers.
The first one contain a reference to the forest automata modelling
the global state, e.g., the block of global variables in the symbolic state.
The second one then represents the allocated memory of the actually executed function.
The local registers serve as temporary memory used in
the microcode instructions for data manipulation.
Putting together the data members of class {\tt SymState}
we can define symbolic state formally as a triple $S=\symstate{F}{\regs}{I}$.

The values of the registers are instances of structure {\tt Data}.
We can formally define structure Data as $\ddata$ where
$T$ is a data type,
$S \in \natnum$ is a size of data,
$V$ is a value of type $T$,
$MB=(D_1 \cdots D_n)$ represents nested data if they are present (this is the case when $D$ represents a data structure)
and $D_i$ is another instance of data.
$T$ could be one of the following data types: pointer, reference to a TA, integer, boolean, structure or some other data type.
$T$ could be also undefined or it could state that type is unknown.
We describe in detail one of the types -- the root reference
whose values are defined as $RR=({\tt Root},{\tt Displ})$
where ${\tt Root} \in \natnum$ is an index of a tree automaton in $F$
and ${\tt Displ} \in \natnum$ is an index of a selector in a label of
the transition from the TA root pointed by ${\tt Root}$.
The operations for manipulation with a symbolic state are partially provided by class {\tt SymState}
and some additional operations are provided by class {\tt VirtualMachine}.

We already described the data structures used for the implementation of the forest automata concepts
and the related symbolic execution.
The symbolic execution using this data structures is realized by gradually appending
and removing the symbolic states from a queue.
When a symbolic state is taken from the queue a microcode instruction contained in
the symbolic state is executed.
Some instructions during its execution appends a new symbolic state to the queue.
Symbolic execution finishes when the queue is empty.
The microcode instructions executed during the symbolic execution are described in the following section.

\subsection{The Microcode Instructions}
\label{subsec:microinstr}

This sections provides a description of the Forester microcode instructions
which correspond to the abstract transformers in context of abstraction interpretation.
Firstly, we introduce some examples how the C statements are translated
to the Forester microcode instructions, then we define the instructions
more formally and we give a complete list of them.

Consider the following data structure that is an implementation of singly-linked list.
	\begin{quote}
	\begin{verbatim}
	struct T {
      struct T* next;
      int data;
	};

	\end{verbatim}
	\end{quote}
	
	Consider also the two variables \code{struct T *x, *y} used in the next examples.
	Note that all constants used in the examples are dependent to a concrete Forester
	run and they are chosen randomly in this text.
	The syntax of instructions in the examples is following:
	$$ op\ r_0,\ r_1,\ r_2$$
	$op$ is a name of a instruction,
	$r_0, r_1, r_2$ are operands such that $r_0$ is a destination register,
	$r_1$ and $r_2$ are the source registers.
	Some instructions do not have three registers as parameters but most of them
	have just two.
	It is also possible that there is a constant instead of a register
	as an operand of the instruction.
	When $[r+c]$ is used as an operand then the instruction dereferences a root reference in register $r$.
	Exactly, the instruction access directly a selector with the displacement $c$ in a label of the transition with
	the referenced root as the parent state.
	Note that when it is not stated otherwise it is referred to a selector in a label of a transition
	where the root of the transition is the referenced TA by the specified register.
	In the following text the registers $r_1,\ldots,r_n$ are local ones and
	$GLOB$ and $ABP$ are global registers such that $GLOB$ register contains reference to a FA representing
	global variables and $ABP$ register contains reference to a FA representing local memory.

\bexmp
	The statement \codix{x = (struct T*) malloc(sizeof(struct T));} is translated
	to these microcode instructions:
	\begin{quote}
	\begin{verbatim}
	1: mov_reg r0, (int)4
	2: alloc r0, r0
	3: node_create r0, r0, next[0:4:+0]
	4: mov_reg r1, ABP + 0
	5: mov_reg [r1 + 12], r0    
	6: check
	\end{verbatim}
	\end{quote}

	Firstly the size of a new allocated data (in this case the size is $4$) is stored to the register $0$ at line~$1$.
	Then the new instance of structure {\tt Data} is created and stored to register $0$ at line $2$.
	A new TA $t$ is created at line $3$.
	The TA $t$ has one transition from the root with a label that consists one pointer selector {\tt next} whose
	displacement in the label is $0$ and the size is $4$.
	The displacement defines the order of the selector in the label.
	The instruction \codix{node} also stores a root reference to $t$ to register $0$.
	Finally lines $4$ a $5$ perform adding root reference pointing to $t$ to a FA that represents
	a memory state of the actually executed function (which is in this case the function {\tt main}).
	In this case a selector related to the pointer x has displacemnt $12$ in the label of TA referenced by the register $1$.
	The instruction at line $6$ then checks whether no garbage was created by the previous instructions.
\eexmp

\bexmp
	The statement \codix{y = x->next;} is symbolically done by the following instructions:
	\begin{quote}
	\begin{verbatim}
	1: mov_reg r0, [ABP + 12]
	2: acc_sel [r0+0]
	3: mov_reg r0, [r0 + 0]
	4: mov_reg r1, ABP + 0
	5: mov_reg [r1 + 16], r0    
	6: check
	\end{verbatim}
	\end{quote}

	The first instruction stores a root reference to an TA representing heap pointed by the \codix{x} pointer to register $0$.
	In this case, a node with a root reference is pointed by the selector with displacement displacement $12$ in a label in TA referenced by register $ABP$.
	The second instruction performs an isolation of the selector with displacement $0$ in the TA referenced by register $0$.
	This causes a creation of a new TA if \codix{x->next} points to an allocated memory (because a new
	cut-point is created when pointer \codix{y} also points to this memory).
	Otherwise the instruction does not do anything.
	The instruction at line $3$ loads to register $0$ a root reference from a selector of TA referenced by register $0$.
	The instruction at line $4$ moves a reference to TA that represents a local memory state to register $1$.
	Finally, the instruction at line $5$ copies the root reference from register $0$ to
	a TA node pointed by pointer \codix{y} selector.
	The last instruction checks whether no garbage is presented.
\eexmp

\bexmp
	The statement \codix{free(x);} is performed by these instructions:
	\begin{quote}
	\begin{verbatim}
	1: mov_reg r0, [ABP + 12]
	2: acca_sel [r0]
	3: free r0
	4: check
	\end{verbatim}
	\end{quote}

	The first instruction loads to register $0$ a root reference to TA representing heap pointed by \codix{x}.
	The reference to the TA is stored in a node pointed by a selector with displacement $12$
	in a label of TA referenced by register \codix{ABP}.
	The second one isolates all selectors of the TA referenced by register $0$
	Then the third instruction removes node pointed by register $0$
	and finally invalidates all references to it.
	The last instruction again checks whether there is not created garbage by performing this instruction.

\eexmp

\bexmp
	The statement \codix{x->data = y->data;} is performed by these instructions:
	\begin{quote}
	\begin{verbatim}
	1: mov_reg r3, [ABP + 16]
	2: acc_sel [r3 + 4]
	3: mov_reg r3, [r3 + 4]
	4: mov_reg r1, [ABP + 12]
	5: acc_sel [r1 + 4]
	6: mov_reg [r1 + 4], r3
	7: check
	\end{verbatim}
	\end{quote}

	The first instruction loads to register $3$ a root reference pointed by a selector (related to pointer \codix{x}).
	with displacement $16$ in TA referenced by register \codix{ABP} (what is TA modeling actually executed function).
	Then the data selector \codix{x->data} (which has displacement $4$ in label in TA reference by register $3$)
	is isolated and a data value from this selector	is loaded to register $r$ by the instructions at line $2$ and $3$.
	The instructions at line $4$ and $5$ prepare data selector \codix{y->data} in the same way
	as the selector \codix{x->data} has been prepared.
	Finally at line $6$ the data value in register $3$ is copied to TA node pointed by
	the selector \codix{y->data}.
	The last operation is again checking whether no garbage left.
\eexmp

\bexmp
	The statement \codix{x == y} translates Forester into this list of the instructions:
	\begin{quote}
	\begin{verbatim}
	1: mov_reg r0, [ABP + 12]
	2: mov_reg	r1, [ABP + 16]
	3: eq r5, r0, r1
	\end{verbatim}
	\end{quote}

	The instruction at line $1$ loads a root reference pointed by a selector
	with displacement $12$ in TA referenced by register \codix{ABP}.
	related to a pointer $x$ to register $0$.
	The second instruction does the same thing as the previous one but for a pointer $y$
	and register $1$.
	The third register then compares the content of the registers $1$ and $2$ and stores the result
	to register $5$.
\eexmp

\vskip 1em

The examples should bring some intuition into the concept of the microcode instructions and
now we give a more detailed and formal description of the instructions.
At first, the notion used further is described.
We use $r_s \in \regs$ to denote the source register,
$r_d \in \regs$ to denote the destination register,
$r_l \in \regs$ to denote a local register and
$r_g \in \regs$ to denote a global register.
Note that the source register is also a global or a local one and the same holds for the destination register.
The source and destination registers are parameters of some instructions.
We use $\regssub{r}{r'}$ to denote a set of the registers which is same as
the set $\regs$ but register $r$ is substituted by register $r'$.
A symbol $I_n$ denotes next instruction following an instruction actually executed.
We define semantics of each instruction formally using a function 
$f: \instrset \times \symset \rightarrow \symset$
where $\symset$ is a set of all symbolic states and $\instrset$ is a set of all instructions.
When it is clear for which instruction is used we omit the first parameter of $f$.
The function $f$ defines an effect of the instruction as transformation of
the current symbolic state $s$ to a new symbolic state $s'$.

A list of the microcode instructions used in Forester with description
of their semantics follows.
Each instruction is described informally first and then its effect is defined
in the notion of the function $f$.
The instructions are divided to the two parts, the first part contains
the instructions modifying forest automata,
the second part the other instructions.

\subsubsection{The Instructions Modifying Forest Automata}
\begin{itemize}
	\item {\tt acc\_sel($r_d$)} isolates the $i$-th selector of a label
		in the transition from the root of a given TA.
		The root of TA is defined by the destination register $r_d$.
		This instruction checks whether a selector pointed by the destination register
		selects a node that represents allocated memory node or undefined pointer.
		In the first case the node is isolated to a new TA and the original node in the
		original TA is replaced by a root reference to the new TA.
		In the second case nothing changes.
		Note that this basically corresponds to identification of the cut-points
		and their separation to a new TA.
		
		$f(\stdsym) = \symstate{F'}{\regs}{I_n}$
		where $F'$ is a new FA obtained from $F$ by isolating
		the $i$-th selector of a label of a transition of the root of TA $t$.
		TA $t$ and the related selector are determined by a root reference $RR=(Root, Displ)$
		in the register $r_d$ such that $Root$ points to $t$ and $i=Displ+o$
		where $o$ is a offset which is a parameter of this instruction.

	\item {\tt acc\_set($r_d$),acc\_all($r_d$)} instruction does the same thing as the previous one
		but for a set of selectors or for all selectors of a given root of TA.
	
	\item {\tt node\_create($r_d$,$r_s$)} instructions creates a new node
		(and hereby also a new TA) which is the root node of the new TA.
		A reference to the new TA is stored to the destination register $r_d$.
		A type info of the new node and its size is obtained from source
		register $r_s$.
		
		$f(\stdsym) = \symstate{F'}{
		\regssub{
			r_{d}}
			x}
		{I_n}$
		where $F'$ is a new FA created by adding newly created TA $t$ to $F$
		and $x$ is a reference to $t$.
		The selectors in the label of $t$ are specified in $r_s$. 
		Note that reference to $t$ is not added to a TA in $F$ in this step
		but it is done by instruction assigning a register value to a
		node of TA.
	
	\item {\tt node\_free($r_s$)} instruction deletes a node in FA (referenced by register $r_s$) and
		invalidates all references to it.
		
		$f(\stdsym) = \symstate{F'}
		{\regs}
		{I_n}$
		where $F'$ is obtained from $F$ by deleting a TA $t$ referenced
		by a value of $r_s$ and removing all reference to $t$ from TA of $F$.

	\item {\tt store($r_d,r_s,o$)} instruction stores a value from the source register $r_s$
		to a TA $t$ pointed by the destination register $r_d$.
		The location is selected by a selector with displacement $o$ in a label of $t$.
		
		$f(\stdsym) = \symstate{F'}{\regs}{I_n}$
		where $F'$ is a FA obtained from $F$ by assignment
		$S_{F[i]}^{displ} = r_{d}$,
		$F[i]$ is a TA of $F$ referenced by a root reference $\rref$ stored in $r_s$ register,
		$o$ is an offset which is a parameter of this instruction,
		$i=\droot$ and $displ=\ddispl + o$.
	
	\item {\tt stores($r_d,r_s,o$)} instruction does the same as the previous
		one but manipulates the structures.
		
		Formal definition is also same but the result is a structure.

	\item {\tt abs(), fix()} instructions computes fixpoint with (abs) or
		without ({\tt fix}) abstraction.
		
		$f(\stdsym) = \symstate{F'}
		{\regs}
		{I_n}$
		where $F'$ is a new FA obtained from $F$ by computing fixpoint with or without abstraction.

	\item {\tt check()} instruction checks whether there is not a garbage
		and alternatively removes all unreachable TA.
		
		$f(\stdsym) = \symstate{F'}
		{\regs}
		{I_n}$
		where $F'$ is obtained from $F$ by removing all unreachable
		TA.

\end{itemize}

\subsubsection{The Instructions Not Modifying Forest Automata}

\begin{itemize}

	\item {\tt alloc($r_d$,$r_s$)} instruction creates a new instance of structure {\tt Data} $\ddata$
		that represents allocated memory.
		The size $S$ of $D$ is determined by an integer value in the source register $r_s$.
		The created instance will be further used for creating of a new TA
		and it is yet stored to the destination register.
		
		$f(\stdsym) = \symstate{F}{
		\regssub{
			r_{d}}
			{x}}
		{I_n}$
		where $x=({\tt void\_ptr},{\tt undef},S',\emptyset)$ is an instance of structure Data
		such that $S'$ is a size stored in $r_s$ (where should be stored an integer value). 

	\item {\tt load\_cst($r_d$,$c$)} instruction loads a constant $c$ to the destination register $r_d$.
		So it creates a new symbolic state which differs only in register content.
		
		$f(\stdsym) = \symstate{F}{\regssub{r_d}{c}}{I_n})$ where
		$c$ is a constant which is parameter of this instruction.
	
	\item {\tt move\_reg($r_d$,$r_s$)} instructions creates a new symbolic state where
		a value from the source register $r_s$ is moved to the destination register $r_d$.
		
		$f(\stdsym) = \symstate{F}{\regssub{r_d}{r_s}}{I_n}$.
	
	\item {\tt bnot($r_d$), inot($r_d$)} instructions negates a value in the destination register and
		creates a new symbolic state with the negated value in the destination register.
		{\tt bnot} negates integer and {\tt inot} negates the boolean value.
		
		$f(\stdsym) = \symstate{F}{\regssub{r_d}{\neg r_d}}{I_n}$.
	
	\item {\tt move\_reg\_offs($r_d$,$r_s$,$o$)} instruction accesses a tree automaton pointed
		by a reference in the source register $r_s$ and increases its displacement value by the given offset $o$.
		The new value is then stored in the destination register $r_d$.
		
		$f(\stdsym) = \symstate{F}{\regssub{r_d}{\rreftuple{\droot}{\ddispl+o}}}{I_n}$
		where $o$ is offset which is a parameter of this instruction
		and $\rrefreg{r_s}$.
	
	\item {\tt move\_reg\_inc($r_d$,$r_{s_1}$,$r_{s_2}$,)} instruction does same thing as the previous one but moreover
		it increases a displacement by a value in the second source register $r_{s_2}$ (which is
		present for this instruction).
		
		$f(\stdsym) = \symstate{F}{\regssub{r_d}{\rreftuple{\droot}{\ddispl + r_{s_2}}}}{I_n}$
		where $r_{s_1} \in \regs, r_{s_2} \in \regs$ are source registers such that
		$\rrefreg{r_{s_1}}$, $r_{s_2}$ is integer value.
	
	\item {\tt get\_greg($r_{ld}$,$r_{gs}$)} loads a value from the global, source register $r_{gs}$ to
		the local, destination register $r_{ld}$.
		
		$f(\stdsym) = \symstate{F}{\regssub{r_{ld}}{r_{gs}}}{I_n}$.
	
	\item {\tt set\_greg($r_{gd}$,$r_{ls}$)} loads a value from the local, source register $r_{ls}$ to
		the global, destination register $r_{gd}$.
		
		$f(\stdsym) = \symstate{F}{\regssub{r_{gd}}{r_{ls}}}{I_n}$.
	
	\item {\tt get\_ABP($r_d$,$GLOB$, $o$)} loads a root reference from the register $ABP$ to
		the destination register $r_d$. The instruction also adds the offset $o$ to a displacement
		in the root reference.

		$f(\stdsym) = \symstate{F}{\regssub{r_d}{
			\rreftuple{\droot}{\ddispl + o}}}
			{I_n}$ where $\rrefreg{r_{ABP}}$ is the register containing root
		reference to FA representing local memory and $o$ is an offset
		which is a parameter of this function.
	
	\item {\tt get\_GLOB($r_d$,$ABP$,$o$)} loads a root reference from the register $ABP$
		to the destination register $r_d$ and adds the offset $o$ to a displacement in the root reference.
		
		$f(\stdsym) = \symstate{F}
			{\regssub{r_{d}}{
				\rreftuple{\droot}{\ddispl + o}
			}}
			{I_n}$ where $\rrefreg{r_{GLOB}}$ is the register containing root
			reference to FA representing the global memory state and $o$ is an offset
			which is a parameter of this instruction.
	
	\item {\tt load($r_d$,$r_s$,$o$)} loads a value from a TA $t$ pointed
		by a root reference in the source register $r_s$ to the destination register $r_d$.
		The value is stored in a selector with displacement $o$ in label of $t$.
		
		$f(\stdsym) = \symstate{F}
			{\regssub{r_{d}}{
				\rreftuple{\droot}{\ddispl + o}
			}}
			{I_n}$
			where $\rrefreg{r_{s}}$,
			$F[i]$ is a TA of F,
			$o$ is offset which is a parameter of this instruction,
			$i=\droot$,
			$displ=\ddispl + o$.

	\item {\tt loads($r_d,r_s,o$)} instruction does the same as the previous	one but manipulates the structures.
		
		Formal definition is the same but the result store in the selector is a structure.

	\item {\tt load\_ABP($r_d$,$r_s$,$o$), load\_GLOB($r_d$,$r_s$,$o$)} instructions do the same as
		the previous instruction but loads a value from a TA pointed by $r_{ABP}$ or $r_{GLOB}$ register.
		
		Formally it could be defined as the previous instruction but
		$ABP$ and $GLOB$ are used instead of $r_s$.

	\item {\tt push\_greg($r_s$)} instruction creates a new global register $r_g$
		and fills it with a value in the source register $r_s$.
		
		$f(\stdsym) = \symstate{F}
		{\regs\cup \{r_g\}}
		{I_n}$
		where $r_g$ is a new global register such that $r_g = r_s$.
	
	\item {\tt pop\_greg($r_g$, $r_s$)} instruction takes a value in the last created
		global register and stores it to the source register.
		The global register is then deleted.
		
		$f(\stdsym) = \symstate{F}
		{\regssubmin{r_g}{r_d}{r_g}}
		{I_n}$
		where $r_g$ is the lastly created global register.

	\item {\tt cond($r_s$,$I_t$,$I_e$)} instruction represents a condition in the original code.
		It creates a new symbolic state with the same forest automaton and appends
		it to the queue.
		The new state contains an instruction which corresponds to a \textit{then} ($I_t$)
		or \textit{else} ($I_e$) branch of the condition.
		The instruction for the new state is chosen according to
		the content of the source register $r_s$.

		$f(\stdsym) = \symstate{F}{\regs}{r_s ? I_t : I_f}$
		where $I_t$ is instruction used when $r_s$ has true value and
		$I_f$ is instruction used otherwise.


	\item {\tt iadd($r_d$,$r_{s_1}$,$r_{s_2}$), imull($r_d$,$r_{s_1}$,$r_{s_2}$)}
		instructions performs integer addition and multiplication of the numbers
		in the source registers $r_{s_1}$ and $r_{s_2}$ and stores the result to
		the destination register $r_d$.
		
		$f(\stdsym) = \symstate{F}
		{\regssub{r_d}{r_{s_1} \otimes r_{s_2}}}
		{I_n}$
		where $r_{s1} \in \regs, r_{s2} \in \regs$, $\otimes \in \{+,*\}$
		are two source registers with values of integer type.

	\item {\tt eq, neq, ge, gt, le, lt}($r_d$,$r_{s_1}$,$r_{s_2}$) instructions
		makes a corresponding comparison of the values in the source registers $r_{s_1}$
		and $r_{s_2}$.
		The result of the comparison stores to the destination register $r_d$.
		
		$f(\stdsym) = \symstate{F}
		{\regssub{r_d}{r_{s_1} \otimes r_{s_2}}}
		{I_n}$
		where $r_{s_1} \in \regs, r_{s_2} \in \regs$ are two source registers
		and $\otimes \in \{=,\neq, <,>,\leq,\geq\}$.
	
	\item {\tt build\_struct($r_d$,$r_{s}$ $i_1 \cdots i_n$)} instruction creates a structure from a
		content of the source registers $r_s,r_{s+i_1},r_{s+i_2}, \ldots, r_{s+i_n}$
		and the result stores to the destination register $r_d$.
		The source registers are defined by an index of the first register and
		a vector of offsets from this register to another registers
		that are used for creating structure.
		
		$f(\stdsym) = \symstate{F}
		{\regssub{r_d}{x}}
		{I_n}$
		where $x$ is a structure created from the values of the registers $r_s,r_{s+i_1},r_{s+i_2}, \ldots, r_{s+i_n}$.
		The values $i_1, i_2, \ldots, i_n$ are offsets defined as the parameters of this instruction.
	
	\item {\tt abort()} instruction aborts program execution.
	
	\item {\tt assert($r_s$,$c$)} instruction checks whether the value
		in the source register is the same one as the constant $c$.
		
		$f(\stdsym) = \stdsym$ if $r_s = c$, otherwise
		symbolic execution is aborted.
		Constant $c$ is a parameter of this instruction.
	
	\item {\tt error()} instruction throws an exception representing
		a local error in program.
	
	\item {\tt noret()} instruction quits program symbolic execution when
		no return function end is reached.

\end{itemize}


\chapter{Forester with VATA}
\label{ch:fova}

This chapter describes a process of porting Forester to \vata, its difficulties and design and it also deals with implementation
itself.

First of all it is important to declare that we use \vata\ implementation of explicit encoding of TA because
it is currently the only one that support the most of needed operations over TA and it is also more efficient than
semi-symbolic encoding for purposes of Forester because no large alphabets are used during the verification procedure
so the advantage of the semi-symbolic encoding would not be fully utilized here.

Forester implementation is currently far from being mature and high structural dependency is
one of its bottlenecks.
So the first thing needed to be done is reducing number of dependencies between classes (described in Section \ref{sec:forester_prep}).
Then it would be possible to apply design pattern \emph{adapter} \cite{gamma95} (described in Section \ref{sec:adapter}) to create
an interface between Forester and \vata\ (implementation itself is described in \ref{sec:fova_impl}).
Applying of adapter design pattern makes possible to include VATA without need of rewriting
Forester to the names of methods and data members used in VATA.
It creates also only one place (particularly adapter class) connecting Forester and VATA instead of
including VATA into many of the Forester classes and so it prevents from creating too strong relation between them.

\section{Forester Refactoring}
\label{sec:forester_prep}

The original implementation of tree automata library has also strong dependencies
the other classes.
Hence it was needed to refactore implementation before it was possible to create adapter class for VATA API.
The core class of original tree automata is class \emph{TA} and there are also related classes,
e.g. class \emph{TT} for transitions representation or class \emph{Antichain} for language inclusion checking using the Antichain algorithm from \cite{tacas10}.
This set of classes realizing original tree automata library will be further referred as \emph{tree automata module}.

The refactoring is mainly based on reduction of a number of data types and data members declared
to be \emph{public} (in sense of the C++ programming language).
This is by exploiting features provided by C++11 \cite{stroustrup13} which brings methods (keyword \emph{auto})
for auto deduction of the data types by compiler.
That provides possibility to make some of data types of the original tree automata module \emph{private}.
\emph{Iterator} is another concept often used in combination with auto deduction of types to reduce the need to export internals of tree automata module.
The combination of these two patterns are used for example when one needs to iterate over all transitions of tree automata or all transitions which
have same state as parent.
These kinds of iterations are quite common in Forester.
Another part of refactoring consists of simple replacing access of the class data members by the corresponding getters and setters method.
Reducing of structural dependencies is also done by emphasis application of \emph{Law of Demeter} \cite{lod89} what practically
means that classes using TA explicitly should implement methods providing information about TA instead of providing instance of TA object itself.
E.g., when one class wants to know whether a given state is final in an tree automaton of an forest automaton then FA implementation should
implement a method providing this information instead of providing access to its TA.
That helps us to make a smaller interface for TA module.
However, also after this refactoring Forester source code is still far from accomplishing the mentioned good practices (and also those not mentioned)
in the whole source code.

\section{Adapter Design Pattern}
\label{sec:adapter}

\emph{Adapter} is structural design patter \cite{gamma95} used for creating interface between classes incompatible classes.
Adapter in notion of UML is shown in Figure \ref{fig:adapter}.
Adapter consists from a class \emph{Adaptee} which is the class that we want to make compatible with
another class \emph{Client} which wants to use the methods of Adaptee.
Class \emph{Adaptor} is the one providing connection between Client and Adaptee.
Adaptor could be implemented as inherited class from Adaptee and employs the concept of inheritance to redirect
method calls to its parent (with some possible preprocessing).
Another possible implementation is composing Adaptee to Adaptor and using Adaptor like an interface to Adaptee.
Adaptor also could add a new method that combines Adaptee methods to achieve wanted operations.

\begin{figure}
	\begin{center}
		\includegraphics[scale=0.5]{fig/adapter.png}
	\end{center}
	\caption{Adapter design pattern expressed in UML.
	Picture is taken from \cite{wiki:adapter}}.
	\label{fig:adapter}
\end{figure}

Let us show what the mentioned roles are in our case.
Adaptee is the API of \vata, specifically it is the class \emph{ExplicitTreeAut} implementing representation of a tree automaton in explicit encoding.
Client is in our case not just one class but it is a set of the Forester classes using TA library.
Adaptor is a newly implemented class \emph{VATAAdapter} which description is in the following section.

\section{Implementation}
\label{sec:fova_impl}

The main part of the adapter patter is in our case newly implemented class \emph{VATAAdapter} playing role of Adaptor.
We decided to used the implementation approach to Adaptor, preferring composition over inheritance,
because it is more suitable for our purposes since we often needs to rename methods 
(name of a method in Forester differs from name of a method in VATA performing same operation)
or convert a data type of parameter of tree automata operation (e.g. from vector to set). 

The class \emph{VATAAdapter} instantiates class \emph{ExplicitTreeAut} from VATA as its private data member
and redirects to this instance method calls from Forester (the names of methods of VATAAdapter are the same as they were
in the original TA library).
\emph{VATAAdapter} also sometimes performs mentioned conversion of the data types.
There are methods implemented by adapter not presented in VATA like method \emph{unfoldAtRoot}
performing some kind of an unfolding.
The methods of this kind are very Forester specific so it is not sensible to add them to general purpose library like VATA is
and it is better to implement them in interface like class VATAAdapter is.

We originally supposed that it would be possible to keep the original TA module along VATA adapter
to be able to easily switch between them.
However it has proved that this would bring high overhead in some situations.
E.g., a~conversion of some data types would be needed in this case
what is overhead compared to implementation where data types compatible with VATA are used directly in the Forester code.
Hence we decided to remove the original tree automata module and further support only version of Forester with \vata.

\chapter{Developing Backward Run and Predicate Abstraction for Forest Automata Based Verification}
\label{ch:backward}

As we already mentioned when an error in analysed program is detected
it is necessary to check whether the error is real one or spurious
so it is created by an overapproximating abstraction over automata.
The analysis of spuriousness is done by \emph{backward run} which has not
yet been designed and implemented in forest automata based verification yet.
When it is proved that the error is spurious we have to refine abstraction to avoid
reaching the error again and restart analysis with the refined abstraction.
This principle of gradually refinement of abstraction is based on CEGAR framework \cite{cegar}.

In this work, we propose using predicate abstraction \cite{artmc} in forest automata based verification
which has more fining tuning then currently used height abstraction.
Predicate abstraction can be refined by obtaining \emph{predicates} from backward run
what guarantees that the spurious error will not be reached again.
However, we restrict ourselves only on basic forest automata, not the hierarchical ones.

The structure of this chapter is following.
The first Section \ref{sec:mybw} describes backward and predicate abstraction run generally
then the second Section contains design of backward run for forest automata.
Section \ref{sec:predicates} provides description of design of
predicate abstraction for forest automata.
Section \ref{sec:isect} describes algorithm for intersection of non-hierarchical forest automata
that is needed for backward run.
Finally, an implementation of both concepts is described at Section \ref{sec:impl}.

\section{Backward Run and Predicate Abstraction}
\label{sec:br}

\begin{figure}[t]
	\centering
	\includegraphics[width=1\linewidth]{fig/artmc.png}
	\caption{
		Figure is taken from \cite{artmc}.
		Figures illustrates principle of forward and backward run and
		a detection of an spurious counterexample.}
	\label{fig:bwrun}
\end{figure}

As we described in Section \ref{ch:fav}, the abstract transformers $\abstr$
related to the concrete program operations are gradually applied to forest automata
starting from a forest automaton modelling an empty heap.
This gradual application of abstract transformers in the same order
with the concrete program operations is called \emph{forward run}.
Formally, a forest automaton at a program point $n$ is obtained by
$\abstr^n(\abstr^{n-1}(\ldots \abstr^{1}(F^{empt}) \ldots))$ in forward run
where $\abstr^i$ is an abstract transformer related to the concrete
program operation at $i-th$ point of program and $F^{empt}$ is
a forest automata modelling an empty (initial) heap.
Moreover, normalization, box folding and abstraction is performed
as a part of abstract transformers at some program locations as
it was described in \ref{ch:fav}.

Consider that a program error is detected at the $n$-th program location
during the forward run.
The verification of non-spuriousness of the error is done by gradual applying
\emph{reverse abstract transformers} $\rabstr$ over forest automata
starting from the point where the error was found
and checking whether intersection of languages of forest automata from forward run
and backward run is non-empty at each program point.
This process is called backward run.
Formally, we compute forest automaton $\rabstr^1(\rabstr^{2}(\ldots \rabstr^{n}(F^{n}) \ldots))$
for each program location from the $n$-th point back to the beginning.
We check whether $\forall 0 \leq i \leq n: L(\rabstr^1(\ldots \rabstr^{i}(F^{i}) \ldots)) \cap
L(\abstr^n(\ldots \abstr^{1}(F^{empt}) \ldots)) \neq \emptyset$
and when the intersection is empty the the found error is declared
to be spurious.

This principle is illustrated by Figure \ref{fig:bwrun}.
Figure shows a forest automata language before (green area)
and after (yellow area) abstraction at a program location where abstraction is performed.
One oval consisting green and yellow part represents language of a forest automaton
in a symbolic state of symbolic execution.
Figure also shows how a forest automaton (and it language) is gradually changed
by abstract transformers what is illustrated by lines connecting ovals.
A~found error is shown as a red area in the last oval.
The backward run starts from this error and it finds
that an error is spurious because an intersection (the orange areas) of
forest automata language from backward (the blue ovals) and forward
(the green and yellow area) run is empty at level of the symbolic state
with language $L(M^{\alpha}_{k-1})$.

So far we describe principle of backward run so we cover
also predicate abstraction that is based on the one introduced
in \cite{artmc}.
Predicate abstraction is not yet used in forest automata based verification
because it looses it advantages over height abstraction without
a backward run which detects the predicates to refine predicate abstraction
in case of a spurious counterexample detection.

Let $\predset=\{\pred_1, \ldots, \pred_n\}$ be a set of predicates.
A predicate $\pred \in \predset$ is a language represented by
a tree automaton.
As we described in Section \ref{subsec:abstraction}, the abstraction function $\abs$
over a set of states $Q$ of TA $A=(Q,\Sigma,\delta, R)$ basically
merges states in the same equivalence class of a equivalence relation.
The predicate abstraction introduces a equivalence relation $\peqrel$
where the states of tree automaton are equivalent when
they have empty intersection with the same predicates.
Formally, $\forall q_1,q_2 \in Q: q_1 \peqrel q_2 \defarrow
(\forall \pred \in \predset: \langstate(A,q_1) \cap \pred \neq \emptyset
\Leftrightarrow \langstate(A, q_2) \cap \pred \neq \emptyset)$.
Note, that predicate abstraction is defined over a TA
and its extension to FA will be described later.

The refinement of predicate abstraction is done by adding
tree automata from a FA in backward run that has empty language
intersection with the related FA from the forward run.
Then the new forward run can be started using the extended predicate
set for the abstraction.
Note that the abstraction merges at most the same states as
in the previous run.
TODO: dukaz, proc je takto dobre.
Mozna neco o terminaci.

\section{Intersection of Forest Automata Languages}
\label{sec:isect}
It is needed to perform the forest automata languages intersection
during the backward run as it was described in the previous section.
We design and implement the algorithm for intersection
of languages of non-hierarchical forest automata because it has not been develop yet.
The algorithm is given at Algorithm \ref{alg:isect}.

The input of the algorithm are two non-hierarchical forest automata $\F_{\A}$
and $\F_{\B}$ and the output is a forest automaton $\F_{\CC}$ such that
$\lang(\F_{\A}) \cap \lang(\F_{\B}) = \lang(\F_{\CC})$.

The algorithm starts with initialized helping data structures and
resulting automaton at lines $3-5$.
Then it iterates over all pointer variables (in symbolic state) of the analysed program
that are visible at the point of program where intersection is performed
and it call the function \emph{intersect} over states of FA pointed
by the variables.

The function intersect takes as the input these states and
it also needs information defining to which TA of FA the state belongs.
The function also manipulates the structures created in the beginning of the
whole algorithm.
It gradually creates the part of the resulting intersection forest automata that starts
from the input states.
The result of this function is index of TA created by the function in the resulting FA.
It process so-called product states during it computation.
A product state is basically a pair $(r,s)$ where $r$ is a state of $\F_{\A}$
and $s$ is a state of $\F_{\B}$.

The function creates a product state from the input states $lhs$ and $rhs$ and
adds it to the set of already processed product states if it is not already processed,
otherwise it returns index of such product state in the result automaton.
This is done at line $1-3$.
Then the counter of processed product states is increased and
stack \emph{workset} is initialized to serve like a auxiliary data
structure where the product states are stored to be processed later at line $4-8$.

The function creates a new TA with the product state $lhs$ and $rhs$ as the root
and with an empty transition relation at line $8$.
Then the product states from the workset are gradually processed at the cycle beginning
at line $9$.
We denote $(r,s)$ the actually processed product state.
There are several cases how the product state is managed.
The first case is when both $r$ and $s$ are the references to other
tree automata.
Then the function intersect is called recursively on the referenced
root states and creates a new TA with index $cp$.
Finally the product state $(r,s)$ is added to the resulting FA
as a root reference to a TA indexed $cp$.
This case is at lines $13-14$.
Another similar cases are when $r$ or $s$ is a root reference and the second one is not.
The function intersect is called with the referenced root state as one parameter
and the second parameter is the state of product state that is not a root reference.
The result of this function call is a index $cp$ of newly created TA.
Then the product state $(r,s)$ is added to the resulting FA as the root reference
to a component indexed by $cp$.
These cases are in algorithm processed at line $16-18$ and $20-22$.
The last one case is when both $r$ and $s$ are not root references
but the standard states of FA.
Then it is iterated over all $a \in \Sigma$ with $\#(a) = n$
and a new transition $t$ is added to the transition relation $\Delta$.
where $t = (r,s) \xrightarrow ((r_1,s_1) \cdots (r_n, s_n))$ such that
$(r_1 \cdots r_n) \in \post{a}{r}$, $(s_1 \cdots s_n) \in \post{a}{s}$ and
$\post{a}{q} = \{(q_1 \cdots q_n) \in Q^n\,|\, q \xrightarrow{a} (q_1 \cdots q_n) \in \Delta\}$.
Finally, when the product state $(r_i,s_i)$ where $1 \leq i \leq n$ is a new one
then it is added to the state set $Q$ and appended to the workset to be further processed.
This last case corresponds to lines $24-30$ in the algorithm.

\input{alg_isect.tex}

\section{Designing Backward Run over Symbolic States}
\label{sec:brdesign}

We described backward run generally in the beginning of this chapter.
Now, a more technical and algorithmically description takes a place.
We recall the definitions given in Section \ref{subsec:microinstr}.

It was mentioned that the tool Forester implements the abstract
transformers as the instructions of its microcode.
The input C program is translated to the list $T=\isfwdp{1}, \isfwdp{2},\ldots, \isfwdp{n}$
of the microcode instructions.
These instruction are executed over a symbolic state of a program
and generates a new symbolic state.
The part of symbolic state are also forest automata representing
the reachable heap configurations in a given symbolic state.

The forward run is done by going through $T$ in the forward order.
The trace of symbolic states $\ftraceseq$ is get by the forward run.
We denote the trace by $\ftrace$ and we call it \emph{forward trace}.
The state $\sfwdp{i}$ is the input of the instruction $\isfwdp{i+1}$
where $i \geq 0$ and the $\sfwdp{i+1}$ is the resulting symbolic state. 

The extension by backward run is quite straightforward.
When an error is detected in a symbolic state $\sfwd{i}$
then the backward run starts from this symbolic state.
The backward run is done by going through $T$ in the reverse order
and an execution of the reverse instruction $\isbwdp{i}$ to each $\isfwdp{i}$
over $\sbwdp{i+1}$ obtaining $\sbwdp{i}$.
We obtain the \emph{backward trace} $\btraceseq$ by the backward run.
We denote the trace by $\btrace$.

In contrast to the conceptual view of the backward run given in Section \ref{sec:br}
it is not necessary to compute FA with the language
$L(\mathcal{F}_i^F) \cap L(\mathcal{F}_i^B)$ directly
by the algorithm for the intersection of forest automata to get the successor
symbolic state in backward run (and also to detect spurious counterexample
by checking whether $L(\mathcal{F}_i^F) \cap L(\mathcal{F}_i^B) = \emptyset$)
after the execution of each instruction in the backward run.
Indeed, it is sufficient to compute intersect FA only for
the microcode instructions {\tt FI\_abs(), FI\_fix()}
because only these instructions performs an abstraction.
The other instruction are precise and elementary enough
that they could reversed without needing to make intersection.

The method of execution of the backward is already described so
the next sections provides the description of the inverse microcode instructions.
We describe each instruction informally first and then the instruction is defined in the notion
of the function $\grev: \grevinter$ which should has the reverse semantics
to the function $f$ from Section \ref{subsec:microinstr}.

\begin{comment}
This section provides a description of the reverse operations for
each instruction of Forester microcode which were introduced in Section \ref{subsec:microinstr}.
We describe each instruction informally first and then the instruction is defined in notion
of the function $\grev: \grevinter$ which should has the reverse semantics to the function $f$ from Section \ref{subsec:microinstr}.
So the function $\grev$ takes as parameter a symbolic state and an instruction and returns a new symbolic state.
All the notion used in the following text is the same as the one defined in Section \ref{subsec:microinstr}.
Moreover we use $\ftrace = \ftraceseq$ to denote a forward trace of the symbolic states explored during
the forward run (note that the trace starts with the leaf state).
The backward trace $\btrace = \btraceseq$ is then created during the backward run.
The symbolic state $\sfwd$ is used for denoting the symbolic state in $\ftrace$ actually visited during the backward run.
The symbolic state $\sbwd$ is the last state created (and appended to $\btrace$) during the backward run.
We denote the instruction related to the state $\sfwd$ by $\isfwd$
and the instruction related to the state $\sbwd$ by $\isbwd$.

The backward run is done by iterating over the $\ftrace$ and for each
instruction related to the selected symbolic states is performed its
reverse operation.
The result of the reverse operation is a new symbol state that is
added to the backward trace $\btrace$.
\end{comment}

\subsubsection{The Instructions With Special Reverse Semantics}

\begin{itemize}
	\item {\tt FI\_node\_create($r_s$, $r_d$)}
		Instruction performs reverse operation to the C function \code{malloc}.
		Instruction gets value from the source register $r_s$ in $\sfwd$.
		If the value is a reference or a null pointer then the new
		state is just copy of $\sbwd$.
		Otherwise if the value type is void pointer then the new symbolic
		state has the same values in the registers as the state $\sbwd$ has but
		the value of the register $r_d$ is substituted by the value $r'_{d}$ in $\sfwd$
		because the register $r_d$ in $\sbwd$ contains reference to a TA $t$
		representing heap pointed by the allocated pointer by this instruction.
		TA $t$ is then removed from FA and all references to it are set invalidated.

		$\grev(\bstdsym) = \symstate{F'}
			{\regssub{r_{d}}{r'_{d}}
			}
			{\isfwd}$
			where $\stdsym = \sbwd$,
			$r_{d} \in \sbwd$, $r'_{d} \in \sfwd$
			and $F'$ is obtained from $T$ by removing TA $t$ referenced by the register $r_d$.

	\item {\tt FI\_node\_free}
		Instruction reverses the effect of freeing memory.
		The removed TA representing a heap pointed by a freed pointer
		is referenced by the root reference $RR$ in the register $r_d$.
		This TA is copied from FA in $\sfwd$ to FA in $\sbwd$ into the
		corresponding position.
		It is also necessary to relabel the selectors of FA in $\sbwd$
		according to FA in $\sfwd$ to replace undefined values created
		after free by the root reference to the renewed FA.

		$\grev(\bstdsym) = \symstate{F'}
			{\regs}
			{\isfwd}$
			where $\stdsym = \sbwd$ and $F'= r(F \cup \{T\})$ where
			$T$ is TA referenced by $RR$ in the register $r_d$ in $\sfwd$
			and $r$ performs relabeling such that $r \xrightarrow{undef} () \in F
			\wedge r \xrightarrow{RR} () \in F''$ then $r \xrightarrow{undef} ()$ is
			replaced by $r \xrightarrow{RR} ()$ and $F''$ is FA of $\sfwd$.

	\item {\tt FI\_store($r_d$)}
		Instruction creates a new symbolic state identical to the $\sbwd$.
		Then the root reference $RR$ is loaded from the register $r_d$ in $\sbwd$.
		The value $V$ from a node pointed by the selector in FA of $\sfwd$ referenced by
		the root reference in register $r_d$ is loaded (which is actually value replaced
		by this instruction in the forward run).
		Finally, the value $V$ is stored to FA of the new symbolic state to the node
		pointed by the selector referenced by $RR$.

		$\grev(\bstdsym) = \symstate{F'}
			{\regs}
			{\isfwd}$
			where $\stdsym = \sbwd$ and $F'$ is obtained
			by the described operation.

	
	\item {\tt FI\_stores}
		This instruction has same reverse operation like the previous one but
		it reverses storing a structure to a FA.

	\item {\tt FI\_abs(), FI\_fix()}
		Reversion is done by creating a new symbolic state identical to the state $\sbwd$.
		Then a new FA created by the intersection of FA of $\sbwd$ and FA of $\sfwd$ is assigned
		to the new symbolic state.

		$\grev(\bstdsym) = \symstate{F'}
			{\regs}
			{\isfwd}$
			where $\stdsym = \sbwd$, $F' = F \cap F_{FWD}$ and $F_{FWD}$ is
			FA of $\sfwd$.

	\item {\tt FI\_push\_greg($r_g$)}
		Instruction creates a new symbolic state identical to the state $\sbwd$ but
		the last global register from the new state is removed (what inverts push operation).

		$\grev(\bstdsym) = \symstate{F}
		{\regs \setminus \{r_g\}}
		{\isfwd}$ where $\sbwd = \stdsym$ and $r_g$
		is the last global register added to the state.

	\item {\tt FI\_pop\_greg($r_g$)}
		Instruction creates a new state identical to the $\sbwd$,
		then the register $r_g$ from $\sbwd$ is moved to the
		forest automaton of the new state (what actually reverts pop)
		and finally the original value of $r_g$ in $\sfwd$ is stored
		to the corresponding register int the new state.
	
		$\grev(\bstdsym) = \symstate{F}
		{\regs \cup \{r_g\}}
		{\isfwd}$ where $\sbwd = \stdsym$ and $r_g$
		is the last global register added to the state.
	
	\item {\tt FI\_set\_greg($r_g$)}
		Instruction creates a new symbolic state identical to the $\sbwd$,
		loads an old value of the global register $r_g$ from $\sfwd$
		and sets the value of $r_g$ in the new state to it.

		$\grev(\bstdsym) = \symstate{F}
		{\regs \cup \{r_g\}}
		{\isfwd}$ where $\sbwd = \stdsym$.

	\item {\tt FI\_abort}
		This instruction is not reversible.

	\item {\tt FI\_acc\_sel}
		Instruction has empty semantics.
		The original instruction isolates a selector
		and so it can create a new TA.
		It is not necessary to reverse this operation
		since it does not change the language of FA of the
		symbolic state or anything else in the symbolic state.
		
		$\grev(\stdsym) = \stdsym$

	\item {\tt FI\_acc\_set,FI\_acc\_all}
		This the same as the previous instruction and so
		the semantic of these instructions is empty.
		
		$\grev(\stdsym) = \stdsym$

\end{itemize}

\subsubsection{Register Assignment Instructions}
The following instructions do simply an assignment to the register.
So reversing them consist creating a symbolic state $S'$ with
the same registers as the symbolic state $\sbwd$ has
and a substitution of the original destination register $r_d$ of $\isfwd$ in $S'$
by the value of the same register in the state $\sfwd$ (denoted by $r'_d$).
Formally, $\grev(\stdsym) = \symstate{F}
			{\regssub{r_{d}}{r'_{d}}
			}
			{I}$
			where $\stdsym = \sbwd$.

\begin{itemize}

	\item {\tt FI\_load\_cst}

	\item {\tt FI\_move\_reg}

	\item {\tt FI\_bnot, FI\_inot}

	\item {\tt FI\_move\_reg\_offs}

	\item {\tt FI\_move\_reg\_inc}

	\item {\tt FI\_get\_ABP}

	\item {\tt FI\_get\_GLOB}

	\item {\tt FI\_load}
	
	\item {\tt FI\_load\_ABP, FI\_load\_GLOB}
	
	\item {\tt FI\_get\_greg}
	
	\item {\tt FI\_loads}
	
	\item {\tt FI\_alloc}

	\item {\tt FI\_iadd, FI\_imull}

	\item {\tt FI\_eq, FI\_neq, FI\_ge, FI\_gt, FI\_le, FI\_lt}
	
	\item {\tt FI\_build\_struct}

\end{itemize}

\subsubsection{Void Instructions}
The following instructions have no reverse semantics
(because they do not change the symbolic state in forward run)
so the symbolic state obtained by the reverse instruction
is the same as the symbolic state $\sfwd$.
Formally written, $\grev(\stdsym) = \stdsym$ where $\sbwd = \stdsym$.

\begin{itemize}
	
	\item {\tt FI\_cond}
	
	\item {\tt FI\_check}
	
	\item {\tt FI\_assert}
	
	\item {\tt FI\_error}
	
	\item {\tt FI\_noret}

\end{itemize}


\section{Designing Predicate Abstraction over Forest Automata}
\label{sec:padesign}

It was already mentioned that predicate abstraction taken from \cite{artmc}
is defined over tree automata.
We decide not to extend the concept completly to forest automata but
to do abstraction over forest automata component wise.
It means that when the abstraction is performed by the microcode instruction
{\tt FI\_abs} it is done for each tree automaton of forest automaton in curreny symbolic
state separetly.
Formally, let $\fpabs: \fclass \rightarrow \fclass$ be an abstract function where
$\fclass$ is a domain of forest automata.
The function $\pabs$ is defined as follows.
Let $F_1=(A_1 \cdots A_n, \pi_A)$ and $F_2=(B_1 \cdots B_n, \pi_B)$ be two forest automata
and let $\pabs$ be an abstraction function merging states of a tree automaton according
to the equivalence relation $\peqrel$,
then $\fpabs(F_1) = F_2 \defarrow \forall i \in \{1,\ldots,n\}: B_i = \pabs(A_i)$.

This also implies that the states are merged only in on tree automata not across the different
tree automata of forest automata.
However, merging states of different tree automata can be more efficient (since
a forest automaton with smaller number of states is created) but this
work porposes the first version of predicate abstraction and more
fine tuning including theoretical correctness of the new method could
be done as the future work.

The predicates are also represented by tree automata.
Furthemore, we use compressive representation of predicates
proposed in \cite{artmc} where the predicates are the langages $\lang(A,q)$
of the states $q \in Q$ of a tree automaton $A=(Q,\Sigma,\Delta, R)$
Checking whether a state of a tree automaton has non-empty intersection
with the same sets of predicates as another state of the automaton is done by the following method.
Consider again TA $A$ and the set of predicates $\predset = \{\pred_1, \ldots, \pred_n\}$
that are represented by set of TA $\{\predaut_1, \ldots, \predaut_m\}$.
Note that one TA can preresent more predicates.
Then we create the product automata $A \times \predaut_1, \ldots, A \times \predaut_m$.
We use the algorithm for the bottom-up tree automata intersection from \cite{mt:lengal}.
Each of product automata $A \times \predaut_i$, where $1 \leq i \leq$, has the state set
containing the product states of the form $(p,q) \in Q_A \times Q_{\predaut_i}$.
Then it is possible to define function $m: Q_A \rightarrow 2^{Q_{\predaut_1} \cup \ldots
\cup Q_{\predaut_m}}$ as follows: $m(p) = \{q \in Q_{\predaut_1} \cup \ldots \cup Q_{\predaut_m}\,|\,
\exists A \times \predaut_i: (p,q) \in Q_{A \times \predaut_i}\}$.
Then we define the equivalence relation $\peqrel \subseteq Q \times Q$
such that $(p_1,p_2) \in \peqrel \defarrow m(p_1) = m(p_2)$.
Then it is possible to merge the states from $A$ according to this equivalence relation
using the predicate function $\fpabs$.

Another thing that is needed to design for the predicate abstraction is collecting a predicates.
When a found error is considered to be a spurious one, then the abstraction needs to refined
by creating the new predicates and adding them to the current set of the predicates $\predset$.
The new predicates are created by the following technique.
Consider a point of an analysed program where the spurious error is detected
because $\lang(F_F) \cap \lang(F_B) = \emptyset$ where FA $F_F$ is obtained
from a symbolic state of the forward run and FA $F_B$ is obtained from a symbolic state of
the backward run.
Then we create a normalized FA $F_F^N = (A_1 \cdots A_n, \pi_A)$ by normalization of $F_F$
and a normalized $F_B^N = (B_1 \cdots B_m, \pi_B)$ by normalization of $F_B$.
It can be supposed that $n = m$ because normalization transforms the both FA in an uniform form.
TODO: MOZNA VIC POPSAT PROC TOTO LZE PREDPOKLADAT
Then the set of the new predicates $\predset_{new} = \{B_i \in F^N_B \,|\, \lang(A_i) \cap \lang(B_i) =
\emptyset\}$ is obtained.
Finally, the symbolic execution is restarted again with the $\predset = \predset \cup \predset_{new}$.

\section{Implementation}
\label{sec:impl}

This section provides description how the described methods have been implemented
to the Forester tool.
We describe the implementation of backward run in Section \ref{subsec:bwimpl}
and the implementation of intersection of symbolic states in Section \ref{subsec:isectimpl}.
As the last one is covered implementation of predicate abstraction in Section \ref{subsec:paimpl}.

\subsection{Backward Run}
\label{subsec:bwimpl}

The backward is possible to turn on by setting the macro {\tt FA\_BACKWARD\_RUN}
in file \emph{config.h} to value $1$.
If this macro is set to value $1$ then on a detected error
the backward run is executed by class \emph{SymExec} that
contains the methods for symbolic execution.

The main method of backward run is called \emph{isSpuriousCE} that
checks whether for a given forward trace is an error at a program point
spurious or not and returns true or false according to it.
This method is implemented in class \emph{BackwardRun} what is wrapping
class of backward run.

As it was desribed backward run is performed by going through the given
forward trace in the backward orded and executing reverse operation to each
microcode instruction.
The method \emph{reverseAndIsect} has been added to class \emph{AbstractInstruction}
for this purpose.
The class \emph{AbstractInstruction} is the parent class of the inheritance hiearchy of
microcode instructions.
So each microcode instruction has to implement this method if it its parent class does not
allready implement it.
The method should changes the symbolic state in way that it reverse the semantic of
the method \emph{execute} that is run in forward run.
Despite the name of the method, the instructions do not need to perform
intersection of forest automata (which is done only the cases described in
previous section).
The method \emph{reverseAndIsect} returns a new symbolic state that
is used as the next symbolic state in the backward run.

The method \emph{isSpuriousCE} returns {\tt false} whenever
a symbolic state return by the method \emph{reverseAndIsect}
contains a forest automaton with empty language.
In such case, it also  creates the new predicates.
When the method reaches the beginning of the forward
trace, then the found error is real and the {\tt true}
is returned.

\subsection{Intersection of Forest Automata}
\label{subsec:isectimpl}

The method \emph{reverseAndIsect} of class \emph{FixpointBase}
performs intersection of forest automata.
This class is inherited by the classes implementing microcode
instructions {\tt FI\_abs, FI\_fix} which both computes fixpoint.
The intersection of forest automata is provided by class \emph{SymState}
representing a symbolic state that contains forest automata
but moreover contains information about variables in the current
symbolic states what is needed by the algorithm for forest automata intersection
from Section \ref{sec:isect}.

The main method for symbolic state intersection is called \emph{Intersect}
which makes an intersection between a symbolic state that calls the method and
another symbolic state which is given as a paramter.
The result of the intersection replaces the content of the calling object.

Another operation implemented for purposes of the backward run is
method \emph{SubstituteRefs} which performs the root references
substitution as it is needed for reversing the instruction {\tt FI\_free}.
The root represent to be substituted and reference for substitution are
given in the parameters of this method.
The method is again member of class \emph{SymState}.

\subsection{Predicate Abstraction}
\label{subsec:paimpl}

The predicate abstraction is implemented by a method \emph{predicateAbstraction}
which is a member of class \emph{Abstraction}.
This method takes a set of tree automata representing the set of predicates
as its parameter.
The abstraction is performed over a forest automaton which is a member of
class \emph{Abstraction}.

The choice of abstraction is done by setting value of macro
{\tt FA\_USE\_PREDICATE\_ABSTRACTION} to value $1$.
When the predicate abstraction is chosen the method \emph{predicateAbstraction}
is called on {\tt FA\_ABS} instruction execution.

The new predicates use by predicate abstraction are created during
the verification whether a found error is spurious or real.
Particularly, the method \emph{isSpuriousCE} creates the predicates
when the spurious error is detected using the method described
in Section \ref{sec:padesign}.
It is necessary to perform the intersection between TA representing
predicates and TA of FA from symbolic state of backward run.
The bottom-up intersection algorithm of tree automata was implemented
to the VATA library for this purpose because VATA contained only the
intersection in top-down way which is less suitable for our purposes since
it generates more product states that are not further needed.

The first run of predicate abstraction should be done
with empty set of predicates what could lead to very
strong abstraction where the most states would be merge.
The result of this would be a lot of spurious counterexamples found
in the first run.
Therefore we use height abstraction with height $1$ (what is
also the original configuration of abstraction used in Forester)
for the first run of symbolic execution.
When a spurious error is detected in this run then the new
predicates are created and the next runs of symbolic execution uses predicate abstraction.


\chapter{Experimental Evaluation}
\label{ch:eval}

This chapter provides exeprimental evaluation of the results of this work.
First, we compare the versions of Forester without and with \vata.
Then we evaluate the backward run on the SV-COMP benchmark
and finally the version of Forester using predicate abstraction is compared to the
original one with height abstraction and also discuss the newly analysis programs.

All of the test cases have been evaluated on the computer with CPU Intel Core $2$ Duo ($2.13$ GHz)
and $4$ GiB memory, using Debian Linux, testing version.
The measured time is the CPU time.
The results of the experiments are the averages of the five measurings.

\section{Forester with VATA Evaluation}

This section provides comparison of the version of Forester tool
that uses its own tree automata library and the version using \vata.
Since the version without VATA is no longer compatible with the
new version of Forester, we can evaluate the performance only
on the programs that are able to analyze also with the older
version of the tool without backward run and predicate abstraction.
Therefore we use height abstraction with the height $1$
what was the original configuration of Forester before the
modifications done as a part of this thesis.

\begin{table}[buh]
	\vskip6pt
	\centering
	\begin{tabular}{|l | c | c |}
		\hline
		Compiler optimization & without VATA & with VATA \\
		\hline
		\hline
		Default    & $40.00$ & $3$ \\
		\hline
		-O3        & $8$ & $3$ \\
		\hline
	\end{tabular}
	\caption{Comparison of Forester with and without VATA on the Forester regress test suite.
	}
	\label{tab:vataregre}
\end{table}

\begin{table}[buh]
	\vskip6pt
	\centering
	\begin{tabular}{|l | c | c |}
		\hline
		Version & without VATA & with VATA \\
		\hline
		\hline
		SLL with CSLL            & x & $5.44$ \\
		\hline
		DLL with CDLL            & x & $10.70$ \\
		\hline
		Skiplist, the 3rd level  & x & $185.70$ \\
		\hline
		Skiplist, the 2nd level  & x & $1.9$ \\
		\hline
		Linux driver snippet     & x & $5.08$  \\ 
		\hline
	\end{tabular}
	\caption{Comparison of Forester with and without VATA on the hard cases.
		Not using compiler optimization.
	}
	\label{tab:vatahc}
\end{table}

\begin{table}[buh]
	\vskip6pt
	\centering
	\begin{tabular}{|l | c | c |}
		\hline
		Version & without VATA & with VATA \\
		\hline
		\hline
		SLL with CSLL            & x & $5.44$ \\
		\hline
		DLL with CDLL            & x & $10.70$ \\
		\hline
		Skiplist, the 3rd level  & x & $185.70$ \\
		\hline
		Skiplist, the 2nd level  & x & $1.9$ \\
		\hline
		Linux driver snippet     & x & $5.08$  \\ 
		\hline
	\end{tabular}
	\caption{Comparison of Forester with and without VATA on the hard cases.
		Using compiler optimization.
	}
	\label{tab:vatahc1}
\end{table}

\section{Backward Run Evaluation}
\label{sec:bweval}

\begin{table}[buh!]
	\vskip6pt
	\caption{sv-comp benchmark evaluation of backward run.%table shows the number of errors confirmed
	}
	\centering
	\begin{tabular}{llr}
		\toprule
		category & real & spurious \\
		\midrule
		heapmanipulation & $8$ & $3$ \\
		memorysafety & $6$ & $0$ \\
		\bottomrule
	\end{tabular}
	\label{tab:bwres}
\end{table}

The implementation of the backward run has been already finished and evaluated on the SV-COMP benchmark.
The implementation of backward run helps in confirmation of the new $6$ real errors in the programs in the memory safety category and
of the new $8$ real errors in the heap manipulation category.
Moreover, the~$3$ spurious counterexamples were detected in heap manipulation category.
The test cases with the~spurious counterexamples will be further resolved by implementing predicate
abstraction.
The results are summarized in Table \ref{tab:bwres}.
The analyzed programs in SV-COMP benchmark includes 
e.g., programs from LDV (Linux Drivers Verification) project
containing implementation of alternating singly-linked list or manipulation with mutex locks, or the programs
implementing bubble sort over a list implementation from the Linux kernel.
Note that Forester does not successfully process all the programs in the test set
because it does not currently support all the C language constructions.
It causes that the number of the found errors, spurious or real, is not as high as it would be when
Forester could analyze all the programs in the benchmark.

\chapter{Conclusion}
\label{ch:concl}

The main goals of this thesis were (a) to implement version of Forester tool that uses the VATA library for tree automata representation and manipulation
and (b) to extend verification procedure based on forest automata with backward run for detection of the spurious errors found in the analysed program.
The theory of forest automata and related theory of tree automata has been studied and described in this thesis and the verification procedure
based on forest automata has been also explored to fulfill the thesis goals.
The connection of Forester and \vata\ was designed and implemented after the analysis of the both tools.
Forester had to be refactored for this purposes.

The first goal has been already reached and the version of Forester using the VATA library successfully participated in competition SV-COMP 2015 \cite{www:svcomp}.
The knowledge about Forester and related verification procedure gained during the work on the term project will be further employed for design
and implementation of backward run in the Forester tool which is the second goal of this master thesis.

